{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC7jZlOZGcUs"
   },
   "source": [
    "# **Task Specific Autoencoder enabled by Tensorflow-Lite Micro end-to-end Tutorial-Deploying to NRF 5340 and OSU Motionsense HRV**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPjMu0MEY9DJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PggJ5kyHGdb1"
   },
   "source": [
    "In this notebook we will show you how to deploy a tensorflow lite micro autoencoder to a cortex-M microcontroller such as the nrf5340 in order to trasmit photoplethysmogram (ppg) IR signals to an mobile device for smooth Heart Rate (HR) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KDoF6b9ZLAI"
   },
   "source": [
    "In the last few years, the power of stronger processors, combined with new techniques for reducing size and cost of machine learning models (a process called quantization) has allowed ai to come all the way down to the internet of things (IoT) level. This is the level of microcontrollers and sensor systems. In other words, with recent proper techniques we can now deploy machine learning on really tiny devices that require very little power, such as watches, security cameras, and wearable medical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybR1g73pbj75"
   },
   "source": [
    "There are many benefits to being able to employ machine learning on IoT devices, and the possibilities are endless. In this tutorial, however, we will be taking a look at deploying a *task specific autoencoder*. Autoencoders are a type of neural network that, given an input, will reduce or compress the size to be very small. Then, they attempt to learn from this compressed signal by trying to recreate it as the output. This allows the model to learn what signals/features are key and should be preserved, and what should be ignored. In our case, we will be using this autoenocoder for feature-preserving compression. That is, we will encode data coming from a ppg sensor and use this to send over Bluetooth Low Energy (BLE) so that this data can be further analyzed by higher caliber and deeper networks with things such as Smooth Heart Rate prediction (HR). BLE is very low-power, and as such, cannot transmit large quanities of data easily such as a full ppg signal. As a result of this, the Autoencoder allows the data to be compressed to such small sizes that only the features needed for learning are kept, and this is a small enough size to be sent over through BLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7m5KuKdt3h"
   },
   "source": [
    "Let us start working on the project itself. You will need:\n",
    "\n",
    "\n",
    "1.   An OSU MotionSenseHRV chip\n",
    "2.   Anaconda or Miniconda installed (See https://docs.anaconda.com/anaconda/install/)\n",
    "\n",
    "After step 2, you'll have conda package manager (for python packages) either through your terminal (Linux, macOS) or through Anaconda Prompt (Windows). Open it in the \"tutorials\" folder as admin/sudo and run the following conda command. These will install all python dependencies in an environment named tf_MSHRV3_AEHR and activate it. Then, re-open this notebook from inside tf_MSHRV3_AEHR environment by simply executing \"jupyter notebook AE_model_tutorial.ipynb\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvCAiyOVPc_v"
   },
   "source": [
    "Keep exp_id='1_1' to reuse existing stored weights and to avoid all training/fitting commands. Keep exp_id to anything else if you want to train from scratch. You can explore commands following \"if exp_id!='1_1'\" in this jupyter notebook to see what steps are skipped when using pretrained weights. The downstream HR prediction model weights are always provided in \"sig2HR\" folder but if it's desired to retrain that model as well, use the separate python script \"sig2HR_model.py\". Ignore most tensorflow WARNINGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to Download the Data via. an automatic python script. Run this only once.\n",
    "\n",
    "from data import get_data\n",
    "get_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjMv94yRWD2T"
   },
   "source": [
    "**Make sure parameters and sample data are in the correct folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcPP2NEjVS17"
   },
   "source": [
    "We will start by getting the necessary model parameters and sample data in order to first test the model. To do this, run the get_data.py file in the repository, or run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zZQ83gIGcl0"
   },
   "outputs": [],
   "source": [
    "# TensorFlow is an open source machine learning library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras is TensorFlow's high-level API for deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Numpy is a math library\n",
    "import numpy as np\n",
    "# Pandas is a data manipulation library \n",
    "import pandas as pd\n",
    "# Scipy is a signal-processing library\n",
    "from scipy.signal import detrend\n",
    "# Matplotlib is a graphing library\n",
    "import matplotlib.pyplot as plt\n",
    "# Math is Python's math library\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#import some custom utility functions\n",
    "from utils import make_data_pipe, Rpeak2HR, sliding_window_fragmentation\n",
    "from sig2HR_model import create_model as create_model_HR\n",
    "from sig2HR_model import create_infer_model as create_infer_model_HR\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "path_prefix='data/pre-training'\n",
    "exp_id='1_2'\n",
    "#current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#log_prefix='../experiments/{}_{}'.format(exp_id,current_time)\n",
    "log_prefix='data/post-training/experiments/{}'.format(exp_id)\n",
    "\n",
    "\n",
    "#path_prefix= 'E:/Box Sync/' #'C:/Users/agarwal.270/Box/' #\n",
    "path=(path_prefix+'/')\n",
    "val_files=[path+'2019092801_3154_clean.csv']\n",
    "test_files=[path+'2019092820_5701_clean.csv']\n",
    "win_len=8 #in sec\n",
    "step=1 #in n_samples\n",
    "Fs_pks=100 #in Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've configured our paths and imports, we should start by first loading our training data for the model, located in the repository as a spreadsheet csv file of a sample recorded ppg signal through time. We do this by using pd.readcsv(), whitch gets our data, and then we reformat and combine using numpy concatenations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uYZnMSSXlkB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_data(path,val_files=[],test_files=[],\n",
    "                   win_len=8,step=1,Fs_pks=100):\n",
    "    '''\n",
    "    Use all files in the folder 'path' except the val_files and test_files\n",
    "    '''\n",
    "    def get_clean_ppg_and_ecg(files):\n",
    "        list_clean_ppg=[];list_arr_pks=[]\n",
    "        for i in range(len(files)):\n",
    "            df=pd.read_csv(files[i],header=None)\n",
    "            arr=df.values\n",
    "            if 'clean' in files[i]:\n",
    "                arr[:,41:45]=(detrend(arr[:,41:45].reshape(-1),0,'constant')\n",
    "                                ).reshape((-1,4))\n",
    "                list_clean_ppg+=[np.concatenate([arr[:,29:31],arr[:,41:45]],\n",
    "                                    axis=-1),arr[:,39:41]]\n",
    "                list_arr_pks+=[arr[:,45:49].reshape(-1)]    \n",
    "        return list_clean_ppg,list_arr_pks\n",
    "    files=glob.glob(path+'*.csv')\n",
    "    #files=[fil for fil in files if 'WZ' in fil] #get wenxiao's data\n",
    "    #separate val and test files\n",
    "    s3=set(files);s4=set(val_files+test_files)\n",
    "    files_2=list(s3.difference(s4))\n",
    "    #files_2=[files_2[0]]\n",
    "    #files_2=[fil for fil in files if not((val_names[0] in fil))]\n",
    "    list_clean_ppg,list_arr_pks=get_clean_ppg_and_ecg(files_2)\n",
    "    \n",
    "    dsample_factr=4\n",
    "    Fs_pks=int(Fs_pks/dsample_factr)\n",
    "    win_len=win_len*Fs_pks\n",
    "    \n",
    "    list_r_pk_locs=[np.arange(len(arr_pks))[arr_pks.astype(bool)] for \n",
    "                    arr_pks in list_arr_pks]\n",
    "    \n",
    "    #get nearest dsampled idx\n",
    "    #TODO: Started using round instead of floor\n",
    "    list_r_pk_locs_dsampled=[np.round(r_pk_locs/dsample_factr).astype(int) for \n",
    "                             r_pk_locs in list_r_pk_locs]\n",
    "    #print([np.max(r_pks) for r_pks in list_r_pk_locs_dsampled])\n",
    "    #print([len(ppg) for ppg in list_clean_ppg[::4]])\n",
    "    \n",
    "    list_arr_pks_dsampled=[]\n",
    "    for j in range(len(list_arr_pks)):\n",
    "        arr_pks_dsampled=np.zeros([int(len(list_arr_pks[j])/dsample_factr),1])\n",
    "        #check & correct for rare rounding up issue in the last element\n",
    "        if list_r_pk_locs_dsampled[j][-1]==len(arr_pks_dsampled):\n",
    "            list_r_pk_locs_dsampled[j][-1]-=1\n",
    "        arr_pks_dsampled[list_r_pk_locs_dsampled[j]]=1\n",
    "        list_arr_pks_dsampled.append(arr_pks_dsampled)\n",
    "    #print([len(ppg) for ppg in list_arr_pks_dsampled])\n",
    "\n",
    "\n",
    "    list_HR=[2*[Rpeak2HR(arr_pks,win_len,step,Fs_pks)] \n",
    "             for arr_pks in list_arr_pks_dsampled]\n",
    "    list_HR=sum(list_HR,[])\n",
    "    #list_HR=[HR[::dsample_factr] for HR in list_HR]\n",
    "    \n",
    "    return list_clean_ppg,list_HR\n",
    "\n",
    "\n",
    "list_sigs,list_HR=get_train_data(path,val_files,test_files,win_len,\n",
    "                                     step,Fs_pks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**import training data from csv file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FA0yDscf9ek"
   },
   "source": [
    "**Convert and Visualize Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPaS6RLufYqH"
   },
   "source": [
    "We will feed the data to the network using tensorflow's [tf.data](https://www.tensorflow.org/guide/data) pipeline which comes with lots of benefits (check out the link to learn more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx6Qza8Vf1jG"
   },
   "outputs": [],
   "source": [
    "#Pre-process data\n",
    "dsample_factr=4;Fs_new=int(Fs_pks/dsample_factr)\n",
    "sample_win_len,step_size=win_len*Fs_new,2*Fs_new\n",
    "HR_win_len=sample_win_len*3 #TODO: Can change this later, 4 is arbitrary choice after profs suggestion\n",
    "ppg_win_len=sample_win_len+HR_win_len\n",
    "\n",
    "model_sigs_in,model_HR_out=[],[]\n",
    "for j in range(len(list_HR)):\n",
    "    #HR=list_HR[j][list_arr_pks[j].astype(bool)]\n",
    "    ppg,HR=list_sigs[j][:,0:2],list_HR[j]\n",
    "    ppg=sliding_window_fragmentation([ppg],ppg_win_len,step_size)\n",
    "    HR=sliding_window_fragmentation([HR],HR_win_len,step_size)\n",
    "    #print(len(ppg),len(HR))\n",
    "    model_sigs_in.append(ppg)\n",
    "    model_HR_out.append(HR[:len(ppg)]) #clipping extra HRs at the end\n",
    "model_sigs_in=np.concatenate(model_sigs_in,axis=0)\n",
    "model_HR_out=np.concatenate(model_HR_out,axis=0)\n",
    "model_in=model_sigs_in#[:,:,0] #removing last dummy dimension\n",
    "model_out=model_HR_out[:,:,0] #removing last dummy dimension\n",
    "print(model_in.shape,model_out.shape)\n",
    "\n",
    "#Visualize our PPG signal\n",
    "idx=1\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.title('A sample PPG and HR')\n",
    "plt.plot(model_in[idx,:,:])\n",
    "plt.ylabel('PPG')\n",
    "plt.grid(True)\n",
    "plt.subplot(212)\n",
    "plt.plot(model_out[idx,:])\n",
    "plt.ylabel('HR (BPS)')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Sample No.')\n",
    "\n",
    "#partition\n",
    "val_perc=0.14\n",
    "val_idx=int(val_perc*len(model_in))\n",
    "\n",
    "#TODO: Changed here to have fused decoder output\n",
    "val_data_AE=[model_in[0:val_idx,:],np.mean(model_in[0:val_idx],axis=-1,keepdims=True)]\n",
    "train_data_AE=[model_in[val_idx:,:],np.mean(model_in[val_idx:],axis=-1,keepdims=True)]\n",
    "val_data_e2e=[model_in[0:val_idx,:],model_out[0:val_idx]]\n",
    "train_data_e2e=[model_in[val_idx:,:],model_out[val_idx:]]\n",
    "\n",
    "train_ds_AE=make_data_pipe(train_data_AE,batch_size=32,shuffle=True)\n",
    "val_ds_AE=make_data_pipe(val_data_AE,batch_size=128,shuffle=False)\n",
    "train_ds_e2e=make_data_pipe(train_data_e2e,batch_size=32,shuffle=True)\n",
    "val_ds_e2e=make_data_pipe(val_data_e2e,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fWoLh6FGbYl"
   },
   "source": [
    "**Loading/Training the model**\n",
    "\n",
    "Now that we have the necessary data in order to properly load, train and test our model, we will start by importing it and then pre processing it.\n",
    "\n",
    "We begin by constructing our machine learning model. Our target network is an AutoEncoder that will encode 8 seconds of a ppg signal into a single 1x16 latent representation (or \"code\"). The layers in between are responsible for the feature-selective compression. If you look at the model, we start with our input, then  convert it into a 256 x 1 layer, with a rectified linear activation function. This is then shrunk to a 64 layer, and then finally our 16 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5lnHXtYlzgm"
   },
   "outputs": [],
   "source": [
    "def create_model_AE(in_shape,latent_shape=(16,),mem_shape=(1,),reps=4,\n",
    "                    encoder=None,decoder=None):\n",
    "    in_shape_AE = (int(in_shape[0]/reps),in_shape[1])\n",
    "    #We will be creating the AE_model here\n",
    "    if encoder is None:\n",
    "        encoder_in = layers.Input(shape=in_shape_AE,name=\"encoder_in\")\n",
    "        x = layers.Reshape((1, in_shape_AE[0], in_shape_AE[1]),\n",
    "            name='expand_dims')(encoder_in) #insert a dummy dim to use conv2d\n",
    "        x = layers.Conv2D(filters=8,kernel_size=(1,3), strides=(1,2),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2D(filters=8,kernel_size=(1,3), strides=(1,2),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2D(filters=16,kernel_size=(1,5), strides=(1,5),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2D(filters=16,kernel_size=(1,5), strides=(1,5),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        #x = layers.GlobalAveragePooling2D(name='GAP{}')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        #x = layers.Dense(256,activation='relu')(encoder_in)\n",
    "        #x = layers.Dense(64,activation='relu')(x)\n",
    "        encoder_out = layers.Dense(latent_shape[0],activation='relu')(x)\n",
    "        encoder = keras.Model(encoder_in, encoder_out, name=\"encoder\")\n",
    "    if decoder is None:\n",
    "        #dec_shape = (latent_shape[0]+mem_shape[0],)\n",
    "        decoder_mem = layers.Input(shape=mem_shape,name=\"decoder_mem\")\n",
    "        decoder_in = layers.Input(shape=(latent_shape[0],),\n",
    "                                  name=\"decoder_in\")\n",
    "        #net_in= tf.keras.layers.Concatenate(axis=-1)([decoder_mem,decoder_in])\n",
    "        x = layers.Dense(units=1*2*latent_shape[0], activation='relu')(decoder_in)\n",
    "        x = layers.Reshape(target_shape=(1, 2, latent_shape[0]))(x)\n",
    "        x = layers.Conv2DTranspose(filters=16,kernel_size=(1,5), strides=(1,5),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2DTranspose(filters=16,kernel_size=(1,5), strides=(1,5),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2DTranspose(filters=8,kernel_size=(1,3), strides=(1,2),\n",
    "                    activation='relu',padding='same')(x)\n",
    "        x = layers.Conv2DTranspose(filters=1,kernel_size=(1,3), strides=(1,2),\n",
    "                    activation='linear',padding='same')(x)\n",
    "                     \n",
    "        #x = layers.Dense(64,activation='relu')(net_in)\n",
    "        #x = layers.Dense(256,activation='relu')(x)\n",
    "        x = tf.squeeze(x, axis=1, name='squeeze_dims') #remove the dummy dim\n",
    "        \n",
    "        x,mem_out = layers.GRU(mem_shape[0], return_sequences=True, \n",
    "                      return_state=True)(x,initial_state=decoder_mem)\n",
    "        decoder_out = layers.Conv1D(filters=1,kernel_size=1, strides=1,padding='same',\n",
    "                             activation=None,name='Conv1')(x)\n",
    "        #decoder_out = layers.Dense(in_shape_AE[0])(x)\n",
    "        decoder = keras.Model([decoder_mem,decoder_in], [mem_out,decoder_out], name=\"decoder\")\n",
    "        decoder.mem_shape = mem_shape\n",
    "    \n",
    "    #return encoder,decoder,decoder\n",
    "\n",
    "    AE_in = keras.Input(shape=in_shape, name=\"AE_in\")\n",
    "    #Functional API is \"call\" function of subclassing API\n",
    "    inputs = layers.Reshape((reps, in_shape_AE[0], in_shape_AE[1]))(AE_in) #Check if reshaping in desired fashion\n",
    "    dec_mem=tf.zeros([tf.shape(inputs)[0],mem_shape[0]])\n",
    "    out_list=[]\n",
    "    for i in range(reps):\n",
    "        z = encoder(inputs[:,i,:,:])\n",
    "        dec_mem,dec_out=decoder([dec_mem,z])\n",
    "        out_list.append(dec_out)\n",
    "        #dec_mem = z[:,-mem_shape[0]:]\n",
    "    out = tf.stack(out_list,axis=1)\n",
    "    out = layers.Reshape((in_shape[0],-1))(out)#Check if reshaping in desired fashion\n",
    "    model_AE = keras.Model(AE_in, out, name=\"AE\")\n",
    "    \n",
    "    return encoder,decoder,model_AE\n",
    "\n",
    "#now we will use this function to make the AE model\n",
    "# Make AE model\n",
    "encoder,decoder,model_AE = create_model_AE(model_in.shape[1:],\n",
    "                                           latent_shape=(16,),\n",
    "                                           reps=4,mem_shape=(4,))\n",
    "#compile AE and prep for training\n",
    "model_AE.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have our auto encoder, let's also connect our smooth heart rate prediction model to the autoencoder to that it can be optimized to be task specific. the HR model will append onto the decoder and find the heart rate from the reconstructed ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing PPG to HR Neural Network model called model_HR for smooth Heart Rate Prediction from one channel PPG signal\n",
    "# Load HR model\n",
    "model_HR = create_model_HR(val_data_AE[1].shape[1:],HR_win_len)\n",
    "weights_dir_HR=log_prefix+'/sig2HR/checkpoints'\n",
    "\n",
    "#copy sig2HR weights to new directory if exp_id!='1_1'\n",
    "if exp_id!='1_1':\n",
    "    os.makedirs(log_prefix,exist_ok=True)\n",
    "    shutil.copytree(log_prefix+'/../1_1/sig2HR', log_prefix+'/sig2HR')\n",
    "    \n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_HR)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_HR.load_weights(latest_ckpt)\n",
    "model_HR.trainable=False #freeze model_HR weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We then stack the model_AE and model_HR together in a model called model_e2e (end2end).\n",
    "def create_model_e2e(in_shape,reps,encoder,decoder,model_HR):\n",
    "    in_shape_AE = (int(in_shape[0]/reps),in_shape[1])\n",
    "    #Put the models together\n",
    "    e2e_in = keras.Input(shape=in_shape, name=\"e2e_in\")\n",
    "    #Functional API is \"call\" function of subclassing API\n",
    "    inputs = layers.Reshape((reps, in_shape_AE[0], in_shape_AE[1]))(e2e_in) #Check if reshaping in desired fashion\n",
    "    dec_mem=tf.zeros([tf.shape(inputs)[0],decoder.mem_shape[0]],name='dec_mem_e2e')\n",
    "    out_list=[]\n",
    "    for i in range(reps):\n",
    "        z = encoder(inputs[:,i,:,:])\n",
    "        dec_mem,dec_out=decoder([dec_mem,z])\n",
    "        out_list.append(dec_out)\n",
    "        #dec_mem = z[:,-decoder.mem_shape[0]:]\n",
    "    out = tf.stack(out_list,axis=1)\n",
    "    out = layers.Reshape((in_shape[0],-1))(out)#Check if reshaping in desired fashion\n",
    "    HR_hat=model_HR(out)\n",
    "    model_e2e = keras.Model(e2e_in, HR_hat, name=\"e2e\")\n",
    "    return model_e2e\n",
    "\n",
    "# Make e2e model\n",
    "model_e2e = create_model_e2e(model_in.shape[1:],4,encoder,decoder,model_HR)\n",
    "model_e2e.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's inspect our model and, finally, train it. The training process may take a while depending on your PC hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model_in.shape is the input format\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(model_AE.summary())\n",
    "print(model_HR.summary())\n",
    "print(model_e2e.summary())\n",
    "\n",
    "ckpt_dir_AE=log_prefix+'/AE/checkpoints'\n",
    "stdout_log_file = log_prefix + '/AE/stdout.log'\n",
    "#ckpt_filepath=ckpt_dir+'/cp-{epoch:04d}.ckpt'\n",
    "ckpt_filepath_AE=ckpt_dir_AE+'/cp-{epoch:04d}.ckpt'\n",
    "# Include the epoch in the file name (uses `str.format`)\n",
    "ckpt_dir_e2e=log_prefix+'/AE/checkpoints_e2e'\n",
    "ckpt_filepath_e2e=ckpt_dir_e2e+'/cp-{epoch:04d}.ckpt'\n",
    "#ckpt_filepath_e2e=ckpt_dir_e2e+'/cp.ckpt'\n",
    "#we will also set a directory to save our trained weights once they are finished\n",
    "weights_path_prefix=log_prefix#glob.glob('../experiments/' + exp_id + '*')[0] #for latest experiment weights\n",
    "\n",
    "\n",
    "os.makedirs(ckpt_dir_AE,exist_ok=True)\n",
    "os.makedirs(ckpt_dir_e2e,exist_ok=True)\n",
    "file_path=log_prefix+'/AE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "#os.makedirs(weights_dir_AE,exist_ok=True)\n",
    "#ckpt_filepath_AE=weights_dir_AE+'/cp.ckpt'\n",
    "\n",
    "if exp_id!='1_1':\n",
    "    #callbacks\n",
    "    callbacks=[]\n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_AE,\n",
    "                    save_weights_only=True,save_best_only=True,\n",
    "                    monitor=\"val_loss\",mode='min'))\n",
    "    # Train\n",
    "    model_AE.fit(train_ds_AE,epochs=600,validation_data=val_ds_AE,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best AE ckpt\n",
    "weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_AE)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "\n",
    "# Train End2End model\n",
    "if exp_id!='1_1':\n",
    "    #callbacks\n",
    "    callbacks=[]\n",
    "\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e,\n",
    "                    save_weights_only=True,save_best_only=True,\n",
    "                    monitor=\"val_loss\",mode='min'))\n",
    "    model_e2e.fit(train_ds_e2e,epochs=1000,validation_data=val_ds_e2e,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTx0MUtQnGC1"
   },
   "source": [
    "**Prepare for Quanitization aware training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXBQMqBMnNuc"
   },
   "source": [
    "Now that we have our model ready, we should now use something called quantization aware training (QAT), that will train our network in preparation to be converted to a lighter format. Everything before this can be run in a tensorflow_GPU conda environment for faster execution. For QAT, we need tf-nightly which is the default installation in tf_MS3 conda environment. \n",
    "\n",
    "Integer Quantization is the process of converting parameters (weights, bias, and activations) in our network to 8 bit integer values, used because 8bit integer values are both faster and lighter in terms of storage, whitch is very helpful for microcontrollers. While QAT is not the quantization process itself, This training will optimize our weights to the 8bit model and fine tune them to prepare for this conversion process. It does this by simulating what the model would run like if it used integers, and then optimizing these pre-converted numbers to minized the loss function AFTER it has been trained. Note that we have not actually converted the numbers by involking QAT, we have just prepared the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba-PeGYWn0J5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmopt\n",
    "\n",
    "#Load best e2e ckpt\n",
    "#latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e)\n",
    "#print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "#model_e2e.load_weights(latest_ckpt)\n",
    "\n",
    "#performing generation of the quantization aware model of the encoder\n",
    "quan_model_generator = tfmopt.quantization.keras.quantize_model\n",
    "encoder_q = quan_model_generator(encoder) #quantization aware encoder\n",
    "decoder_noq = keras.models.clone_model(decoder)\n",
    "decoder_noq.mem_shape=decoder.mem_shape #copy mem_shape attribute explicitly\n",
    "decoder_noq.set_weights(decoder.get_weights())\n",
    "decoder_noq.trainable=False #Freeze decoder for now\n",
    "\n",
    "# Make the AE model, for quantization of it later on\n",
    "\n",
    "_,_,model_AE_q = create_model_AE(model_in.shape[1:],\n",
    "latent_shape=(16,),reps=4,mem_shape=(4,),encoder=encoder_q,decoder=decoder_noq)\n",
    "\n",
    "# Make e2e model\n",
    "model_e2e_q = create_model_e2e(model_in.shape[1:],4,encoder_q,decoder_noq,model_HR)\n",
    "\n",
    "\n",
    "#model_in.shape is the input format, you should see (0, 16)\n",
    "print(encoder_q.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Train QAT model**\n",
    "\n",
    "The QAT version of our model has now been configured properly and is ready to 8-bit optimize, by training. We train this model in the same fashion that we trained our original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_e2e_q.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "#we set the directory to our pretrained path\n",
    "weights_dir_e2e_q = weights_path_prefix+'/AE/checkpoints_e2e_q'\n",
    "os.makedirs(weights_dir_e2e_q,exist_ok=True)\n",
    "ckpt_filepath_e2e_q=weights_dir_e2e_q+'/cp.ckpt'\n",
    "\n",
    "# Train End2End model\n",
    "if exp_id!='1_1':\n",
    "    callbacks=[]\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e_q,\n",
    "                    save_weights_only=True,save_best_only=True,\n",
    "                    monitor=\"val_loss\",mode='min'))\n",
    "    # Train\n",
    "    model_e2e_q.fit(train_ds_e2e,epochs=50,validation_data=val_ds_e2e,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgqBJSCzoryb"
   },
   "source": [
    "**Compare QAT results with normal model**\n",
    "We also should make sure that most of our accuracy is preserved during this conversion process. Here we check only using validation set but we encourage and leave this verification, using the test data, as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "rmse=lambda y,y_hat:np.sqrt(np.mean((y.reshape(-1)-y_hat.reshape(-1))**2))\n",
    "\n",
    "# Predictions from AE\n",
    "weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_AE)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "AE_model_mse = model_e2e.evaluate(val_ds_e2e, verbose=1)\n",
    "\n",
    "# Predictions for new e2e AE    \n",
    "weights_dir_e2e=weights_path_prefix+'/AE/checkpoints_e2e'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "e2e_model_mse = model_e2e.evaluate(val_ds_e2e, verbose=1)\n",
    "\n",
    "#Load best e2e_q checkpoint, training by the model\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e_q)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_e2e_q.load_weights(latest_ckpt)\n",
    "q_aware_model_mse = model_e2e_q.evaluate(val_ds_e2e, verbose=1)\n",
    "\n",
    "print('Baseline AE val mse on HR:', AE_model_mse)\n",
    "print('AE val mse on HR after end-to-end training:', e2e_model_mse)\n",
    "print('End-to-end trained AE val mse on HR after Quantization:', q_aware_model_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference model\n",
    "pred_tsteps=decoder.outputs[1].shape.as_list()[1]\n",
    "infer_model_HR=create_infer_model_HR([pred_tsteps,decoder.outputs[1].shape.as_list()[2]])\n",
    "# Load HR model\n",
    "weights_dir_HR=weights_path_prefix+'/sig2HR/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_HR)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "infer_model_HR.load_weights(latest_ckpt)\n",
    "\n",
    "#Check if weights correctly loaded in infer model\n",
    "HR_weights=[v.numpy() for v in model_HR.variables]\n",
    "infer_HR_weights=[v.numpy() for v in infer_model_HR.variables]\n",
    "HR_weights_check=[(HR_weights[i]==infer_HR_weights[i]).astype(int).reshape(-1).prod()\n",
    "                  for i in range(len(HR_weights))]\n",
    "print(HR_weights_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(file_path,win_len,step,Fs_pks):\n",
    "    df=pd.read_csv(file_path,header=None)\n",
    "    arr=df.values\n",
    "    test_in=[np.concatenate([arr[:,29:31],arr[:,41:45]],axis=-1),\n",
    "                            arr[:,39:41]]\n",
    "    \n",
    "    arr_pks=arr[:,45:49].reshape(-1)\n",
    "    \n",
    "    dsample_factr=4\n",
    "    Fs_pks=int(Fs_pks/dsample_factr)\n",
    "    win_len=win_len*Fs_pks\n",
    "    \n",
    "    r_pk_locs=np.arange(len(arr_pks))[arr_pks.astype(bool)]\n",
    "    \n",
    "    #get nearest dsampled idx\n",
    "    #TODO: Started using round instead of floor\n",
    "    r_pk_locs_dsampled=np.round(r_pk_locs/dsample_factr).astype(int)\n",
    "    #print([np.max(r_pks) for r_pks in list_r_pk_locs_dsampled])\n",
    "    #print([len(ppg) for ppg in list_clean_ppg[::4]])\n",
    "    arr_pks_dsampled=np.zeros([len(test_in[0]),1])\n",
    "        #check & correct for rare rounding up issue in the last element\n",
    "    if r_pk_locs_dsampled[-1]==len(arr_pks_dsampled):\n",
    "        r_pk_locs_dsampled[-1]-=1\n",
    "    arr_pks_dsampled[r_pk_locs_dsampled]=1\n",
    "    #print([len(ppg) for ppg in list_arr_pks_dsampled])\n",
    "\n",
    "\n",
    "    list_HR=2*[Rpeak2HR(arr_pks_dsampled,win_len,step,Fs_pks)] \n",
    "    \n",
    "    test_in=[ppg.astype('float32') for ppg in test_in]\n",
    "    test_out=[HR[:,0].astype('float32') for HR in list_HR]\n",
    "    return test_in,test_out\n",
    "\n",
    "#load test data\n",
    "ppg_in,HR_out=get_test_data(test_files[0],win_len,step,Fs_pks)\n",
    "ppg,HR=ppg_in[0][:,0:2],HR_out[0]\n",
    "\n",
    "ppg=sliding_window_fragmentation([ppg],pred_tsteps,pred_tsteps)\n",
    "HR=sliding_window_fragmentation([HR],pred_tsteps,pred_tsteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Some more Plotting and evaluation\n",
    "\n",
    "# Predictions from AE\n",
    "weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_AE)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "\n",
    "\n",
    "dec_mem=np.zeros([1,decoder.mem_shape[0]],dtype=np.float32)\n",
    "HR_mem=np.zeros([1,*infer_model_HR.inputs[0].shape.as_list()[1:]],dtype=np.float32)\n",
    "\n",
    "ppg_out_list=[];HR_out_list=[]\n",
    "for i in range(ppg.shape[0]):\n",
    "    z = encoder.predict(ppg[i:i+1,:,:])\n",
    "    #dec_out=decoder.predict([dec_mem,z[:,:-decoder.mem_shape[0]]])\n",
    "    dec_mem,dec_out=decoder.predict([dec_mem,z])\n",
    "    #HR_mem is updated alongwith prediction\n",
    "    HR_mem, HR_out = infer_model_HR.predict([HR_mem,dec_out]) \n",
    "    #dec_mem = z[:,-decoder.mem_shape[0]:] #update dec_mem\n",
    "\n",
    "    ppg_out_list.append(dec_out[0])\n",
    "    HR_out_list.append(HR_out[0])\n",
    "\n",
    "ppg_hat_AE=np.concatenate(ppg_out_list,axis=0)\n",
    "HR_hat_AE=np.concatenate(HR_out_list[1:],axis=0)\n",
    "\n",
    "# Predictions for new e2e AE    \n",
    "weights_dir_e2e=weights_path_prefix+'/AE/checkpoints_e2e'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "\n",
    "dec_mem=np.zeros([1,decoder.mem_shape[0]])\n",
    "HR_mem=np.zeros([1,*infer_model_HR.inputs[0].shape.as_list()[1:]])\n",
    "\n",
    "ppg_out_list=[];HR_out_list=[]\n",
    "for i in range(ppg.shape[0]):\n",
    "    z = encoder.predict(ppg[i:i+1,:,:])\n",
    "    dec_mem,dec_out=decoder.predict([dec_mem,z])\n",
    "    #HR_mem is updated alongwith prediction\n",
    "    HR_mem, HR_out = infer_model_HR.predict([HR_mem,dec_out]) \n",
    "    #dec_mem = z[:,-decoder.mem_shape[0]:] #update dec_mem\n",
    "\n",
    "    ppg_out_list.append(dec_out[0])\n",
    "    HR_out_list.append(HR_out[0])\n",
    "\n",
    "ppg_hat_e2e=np.concatenate(ppg_out_list,axis=0)\n",
    "HR_hat_e2e=np.concatenate(HR_out_list[1:],axis=0)\n",
    "\n",
    "#Get HR from True ppg\n",
    "HR_mem=np.zeros([1,*infer_model_HR.inputs[0].shape.as_list()[1:]])\n",
    "HR_out_list=[]\n",
    "for i in range(ppg.shape[0]):\n",
    "    #HR_mem is updated alongwith prediction\n",
    "    HR_mem, HR_out = infer_model_HR.predict([HR_mem,ppg[i:i+1,:,0:1]])#TODO: selected first LED channel for now\n",
    "    HR_out_list.append(HR_out[0])\n",
    "HR_from_ppg=np.concatenate(HR_out_list[1:],axis=0)\n",
    "\n",
    "target=np.mean(ppg,axis=-1,keepdims=True).reshape(-1)\n",
    "plt.figure()\n",
    "plt.plot(target.reshape(-1),'b',\n",
    "         ppg_hat_AE[...,0].reshape(-1),'r--',\n",
    "         ppg_hat_e2e[...,0].reshape(-1),'g-.')\n",
    "plt.legend(['Target','AE_out','AE_out_e2e'])\n",
    "plt.ylabel('PPG magnitude')\n",
    "plt.xlabel('Sample No.')\n",
    "plt.title('AE_out_rmse={:.2f}, AE_out_e2e_rmse={:.2f}'.format(\n",
    "            rmse(ppg[...,0:1],ppg_hat_AE),rmse(ppg[...,0:1],ppg_hat_e2e)))\n",
    "\n",
    "#HR plot\n",
    "plt.figure()\n",
    "ax1=plt.subplot(311)\n",
    "plt.plot(HR.reshape(-1),'b',HR_from_ppg.reshape(-1),'r--')\n",
    "plt.legend(['True','HR_ppg_true'])\n",
    "plt.subplot(312,sharex=ax1,sharey=ax1)\n",
    "plt.plot(HR.reshape(-1),'b',HR_hat_AE.reshape(-1),'r--')\n",
    "plt.legend(['True','HR_hat_AE'])\n",
    "plt.subplot(313,sharex=ax1,sharey=ax1)\n",
    "plt.plot(HR.reshape(-1),'b',HR_hat_e2e.reshape(-1),'r--')\n",
    "plt.legend(['True','HR_hat_e2e'])\n",
    "plt.xlabel('Sample No.');plt.ylabel('PPG magnitude')\n",
    "plt.suptitle('RMSE: HR_ppg_true={:.4f}, HR_hat_AE={:.4f}, '\n",
    "             'HR_hat_e2e={:.4f}'.format(rmse(HR,HR_from_ppg),\n",
    "                rmse(HR,HR_hat_AE),rmse(HR,HR_hat_e2e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APir38KsqMIN"
   },
   "source": [
    "**Quantize the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc_1HxR6qTbT"
   },
   "source": [
    "We are now fully ready to convert the model into it's lightweight component, an 8 bit fixed point version of our model. Normally, in machine learning models we use a floating point 32 type of number, whitch is a number that supports decimal points and has a high precision amount, meaning you can type a lot of numbers into it. 8 bit integers only support up to 256 possible numbers, and as such, are not as accurate, but are much faster to run. After running this code, a model named according to the string in \"filename\". Quantization is typically done by representing the number by this format: Real number = most_significant_digits_of_real_number*scaling_factor. This is the esense of what the converter is doing, though there are many optimizations you can make to ensure this runs with optimal accuracy. You can do this by modifying the attributes of the converter objects, as you can see in the code below. However, since we have employed Quantization Awareness training, most of these settings are automated, and we just need to make sure only one flag, converter.optimizations, is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw0wwQXdq3oP"
   },
   "outputs": [],
   "source": [
    "file_name = \"tf_lit2_encoder.tflite\"\n",
    "#prepare to convert the encoder model for deeployment by getting our converter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(encoder_q)\n",
    "\n",
    "\n",
    "#the following optimizations are already configured by QAT:\n",
    "#converter.representitive_dataset\n",
    "#converter.target_spec\n",
    "#converter.inference_input_type and converter.inference_output_type\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "#this converts the model to it's 8 bit weight, bias, and activation form\n",
    "encoder_tf_lite = converter.convert()\n",
    "\n",
    "#we now have our model, and we can save it to our directory for deployment\n",
    "open(file_name, \"wb\").write(encoder_tf_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjHfb6pfrFHQ"
   },
   "source": [
    "**Compare our model to the original**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA8oY_fhthJ8"
   },
   "source": [
    "To make sure our model works, we will now test it with the baseline model and compare mean-sqaure error values to measure how accurate they are. In Tensorflow, you can to this with model.evaluate, but because we are now also working with a tensorflow lite model format we need to pass induvidual values to it through a loop and then stack the values with numpy. Autoencoders work just like any other network in that you can compare the results of the network's output with the desired result, in this case our real ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwh43oVM6PkC"
   },
   "outputs": [],
   "source": [
    "test_tf_lite_model = tf.lite.Interpreter(\"tf_lit2_encoder.tflite\")\n",
    "#you can change the name of this model in the program by renaming the \"file_name\" variable and then\n",
    "#replaceing the above string with your file_name variable\n",
    "test_tf_lite_model.allocate_tensors()\n",
    "#the model involking actually works the same way in tflite micro, the only difference is the language\n",
    "input_tensor = test_tf_lite_model.get_input_details()[0][\"index\"]\n",
    "output_tensor = test_tf_lite_model.get_output_details()\n",
    "\n",
    "repre_data_set = np.array(val_data_AE[1])\n",
    "np.random.shuffle(repre_data_set)\n",
    "\n",
    "result_difference = 0\n",
    "results = []\n",
    "sample_test_num = 3493\n",
    "first_time = False\n",
    "# Run the model's interpreter for each value and store the results in arrays\n",
    "for i in range(0, sample_test_num):\n",
    "    calibrat_data = repre_data_set[i]\n",
    "    calibrat_data = np.reshape(calibrat_data, (1, 400))\n",
    "    tensor_calibrat = tf.convert_to_tensor(calibrat_data, dtype=tf.float32)\n",
    "    test_tf_lite_model.set_tensor(input_tensor, tensor_calibrat)\n",
    "    test_tf_lite_model.invoke()\n",
    "    output_data = test_tf_lite_model.get_tensor(output_tensor[0]['index'])\n",
    "    normal_model_output = 0\n",
    "    if not first_time:\n",
    "        result = np.array(output_data)\n",
    "        first_time = True\n",
    "        result_parent = np.array(normal_model_output)\n",
    "    else:\n",
    "        result = np.vstack((result, output_data))\n",
    "        result_parent = np.vstack((result_parent, normal_model_output))\n",
    "\n",
    "    #now we have the data from both sets encoder models. All we need to do is pass them through data\n",
    "\n",
    "    #now we are just taking the average of the difference of each value in the output tensor and printing this\n",
    "    result_arr = np.subtract(normal_model_output, output_data)\n",
    "    result_arr = np.absolute(result_arr)\n",
    "    result_difference = np.mean(result_arr)\n",
    "    #we are taking a mean here to compare the result_difference)\n",
    "    results.append(float(result_difference))\n",
    "\n",
    "final_diff = np.mean(results)\n",
    "result_parent = encoder.predict(repre_data_set)\n",
    "decoder.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "print(\"the mean ouput loss across\", sample_test_num, \"runs is \", final_diff)\n",
    "tflite_loss = decoder.evaluate(result, repre_data_set)\n",
    "parent_loss = decoder.evaluate(result_parent, repre_data_set)\n",
    "model_AE.evaluate(repre_data_set, repre_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64zKtXlvMHWY"
   },
   "source": [
    "**Convert the model to a Tflite micro model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjlVHTr5NKtu"
   },
   "source": [
    "Now that we have our full tensorflow lite model, let's convert this model into a tf-lite-micro file so that it can be deployed directly to a micro-controller. You can do this by opening up linux and running the following commands in the terminal. Make sure you replace the MODEL_TFLITE and MODEL_TFLITE_MICRO with the tflite file name and the converted name you want the file to be, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Install xxd if it is not available\n",
    "!apt-get update && apt-get -qq install xxd\n",
    "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "# Update variable names\n",
    "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Apb78TZSZIG"
   },
   "source": [
    "**load a tensorflow nrf-project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a--Qwq0XS6VO"
   },
   "source": [
    "This is where our project gets a little specific. Using your MotionSense Device, connect it to your computer. You should see a notification that Jlink is connected. Next, make sure you install nrf connect sdk, and get the segger embedded studio version. You should follow the intruction guide as specified in this [nordic guide](https://developer.nordicsemi.com/nRF_Connect_SDK/doc/latest/nrf/gs_installing.html), following the linux intructions. Once you are in Segger Embedded Studio, click on file:open Nrf Connect Project. Next, grab the project from the repository (located in edge-deployment). In the src tab, click on the (...) icon and select the project folder. In the board tab, select nrf5340pdk_nrf5340_cpuapp. This should load the project into the Segger IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update the model with newly created one**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the project has opened, you should see a file named model.cc. This is the current model that the application is using, and we will need to replace this with our own model. In the project directory, go to the CmakeLists.txt file and open it. You should see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```cmake\n",
    "cmake_minimum_required(VERSION 3.13.1)\n",
    "find_package(Zephyr HINTS $ENV{ZEPHYR_BASE})\n",
    "project(external_lib)\n",
    "\n",
    "#target_sources(app PRIVATE src/hello_world_test.cc)\n",
    "target_sources(app PRIVATE src/main.cc)\n",
    "target_sources(app PRIVATE src/main_functions.cc)\n",
    "target_sources(app PRIVATE src/constants.cc)\n",
    "target_sources(app PRIVATE src/output_handler.cc)\n",
    "target_sources(app PRIVATE src/Tencoder_data.cc)\n",
    "target_sources(app PRIVATE src/assert.cc)\n",
    "target_sources(app PRIVATE src/bluetooth_func.c)\n",
    "target_sources(app PRIVATE src/sample_test.cc)\n",
    "\n",
    "zephyr_include_directories(src)\n",
    "\n",
    "\n",
    "#this file is the target c makelists file for the Ohio State University SENSE lab ble autoencoder.\n",
    "\n",
    "#zephyr_cc_option(-lstdc++)\n",
    "\n",
    "# The external static library that we are linking with does not know\n",
    "# how to build for this platform so we export all the flags used in\n",
    "# this zephyr build to the external build system.\n",
    "#\n",
    "# Other external build systems may be self-contained enough that they\n",
    "# do not need any build information from zephyr. Or they may be\n",
    "# incompatible with certain zephyr options and need them to be\n",
    "# filtered out.\n",
    "\n",
    "#If nithin plans to work on this code we will need to add some if then statements for\n",
    "#controlling these marcos.\n",
    "                # you need to fix the following path before you run the code\n",
    "set(TF_SRC_DIR  /home/devan/Documents/ncs/nrf/applications/nrf-tensorflow/tensorflow)\n",
    "#set(TF_SRC_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../tensorflow)    #Somehow this does not work, and I have no idea why CMAKE does not like the relative path.\n",
    "set(TF_MAKE_DIR ${TF_SRC_DIR}/tensorflow/lite/micro/tools/make)\n",
    "set(TF_LIB_DIR ${TF_MAKE_DIR}/gen/${TARGET}_${TARGET_ARCH}/lib)\n",
    "# Create a wrapper CMake library that our app can link with\n",
    "add_library(tf_lib STATIC IMPORTED GLOBAL)\n",
    "\n",
    "set_target_properties(tf_lib PROPERTIES IMPORTED_LOCATION ${CMAKE_SOURCE_DIR}/lib/nrf5340_cortex-m33_libtensorflow-microlite.a)\n",
    "\n",
    "#set_target_properties(tf_lib PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${TF_SRC_DIR};${TF_SRC_DIR}/tensorflow/lite/micro;${TF_MAKE_DIR}/downloads/flatbuffers/include\")\n",
    "\n",
    "target_link_libraries(app PUBLIC tf_lib)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To link your own model, replace the file in the code marked\n",
    "`target_sources(app PRIVATE src/Tencoder_data.cc)`\n",
    "with your own generated file from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Inspect the code for the tensorflow lite micro model startup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inside the Embedded studio Project you will find a file labeled main_functions.cc. When you run run the project on your device, this is the main stack where most of the tensorflow inference is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```cpp\n",
    "tflite::ErrorReporter *error_reporter = nullptr;\n",
    "const tflite::Model *model = nullptr;\n",
    "tflite::MicroInterpreter *interpreter = nullptr;\n",
    "TfLiteTensor *input = nullptr;\n",
    "TfLiteTensor *output = nullptr;\n",
    "int inference_count = 0;\n",
    "TfLiteIntArray* n_dims;\n",
    "\n",
    "/*\n",
    "There is code normlly here but it is deleted because we are going to\n",
    "eventually change it to be better.\n",
    "\n",
    "\n",
    "/*\n",
    "\n",
    "outputInit();\n",
    "\n",
    "\n",
    "\tstatic tflite::MicroErrorReporter micro_error_reporter;\n",
    "\terror_reporter = &micro_error_reporter;\n",
    "\n",
    "\t// we are creating the model here. This doesn't involve any\n",
    "\t// copying or parsing, it's a very lightweight operation.\n",
    "\tmodel = tflite::GetModel(tf_lit2_encoder_tflite);\n",
    "\n",
    "\tif (model->version() != TFLITE_SCHEMA_VERSION) {\n",
    "\t\tTF_LITE_REPORT_ERROR(\n",
    "\t\t\terror_reporter,\n",
    "\t\t\t\"Model provided is schema version %d not equal \"\n",
    "\t\t\t\"to supported version %d.\",\n",
    "\t\t\tmodel->version(), TFLITE_SCHEMA_VERSION);\n",
    "\t\treturn;\n",
    "\t}\n",
    "\n",
    "\t/* This pulls in all the operation implementations we need. However we might change\n",
    "       this later on as we only really need dense and convulution functions for the autoencoder\n",
    "       But, if we want models to be able to be swapped out during runtime, it should stay this way.*/\n",
    "\n",
    "\t// NOLINTNEXTLINE(runtime-global-variables)\n",
    "\tstatic tflite::AllOpsResolver resolver;\n",
    "\n",
    "\t//This creates our interpreter that runs the (soon to be) auto encoder model.\n",
    "\tstatic tflite::MicroInterpreter static_interpreter(model, resolver,\n",
    "\t\t\t\t\t\t\t   tensor_arena,\n",
    "\t\t\t\t\t\t\t   kTensorArenaSize,\n",
    "\t\t\t\t\t\t\t   error_reporter);\n",
    "\tinterpreter = &static_interpreter;\n",
    "\n",
    "\t// Allocate memory from the tensor_arena for the model's tensors.\n",
    "\tTfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
    "\n",
    "\tif (allocate_status != kTfLiteOk) {\n",
    "\t\tTF_LITE_REPORT_ERROR(error_reporter,\n",
    "\t\t\t\t     \"AllocateTensors() failed\");\n",
    "\n",
    "\n",
    "\t\treturn;\n",
    "\t}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at this code, we have the following. First, we setup all of our variables. The main objects we have are the error reporter and interpreter, whitch are static global objects stored as in our heap, so that it will be permanently in memory. The error reporter object attaches to the interpreter, and will evalute the interpreter closely to make sure it running inferences accuratly and not messing up or erroring. Then, our model object is created, whitch is actually just the model we imported. This model object is composed of all the weights and bias from out parent model, as well as information about the layers, whitch is all encoded in a c byte array that you can see in your created tflite micro file. At runtime, the interpreter interprets these bytes and with the method AllocateTensors(), it makes allocation requests for memory to store the entire working model in. With this, we are ready to be able to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify the dimentions for the tensorflow lite micro inputs and outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you initially run it with Segger Embedded Studio, you should see 2 print statements. These are for verification that the model is working properly. The second statement prints out the input and output tensors dimentions. Verify that these are correct with the model that you imported. Also check that this matches the code to place in the input tensors in. The Snippet looks like this:\n",
    "\n",
    "```cpp\n",
    "//get ready to load the test ppg data into the input tensor\n",
    "        for (int i = 0; i<400; i++){\n",
    "                input->data.f[i] = sample_data[i];\n",
    "        }\n",
    "        float test_bounds = input->data.f[399];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And should match the number in the input tensor print statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Receiving the setup data on target device**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZwF0gRz0eOx6JSY/eOPP9",
   "name": "AE_model_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
