{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC7jZlOZGcUs"
   },
   "source": [
    "# **Task Specific Autoencoder enabled by Tensorflow-Lite Micro end-to-end Tutorial-Deploying to NRF 5340 and OSU Motionsense HRV**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPjMu0MEY9DJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PggJ5kyHGdb1"
   },
   "source": [
    "In this notebook we will show you how to deploy a tensorflow lite micro autoencoder to a cortex-M microcontroller such as the nrf5340 in order to trasmit photoplethysmogram (ppg) IR signals to an mobile device for smooth Heart Rate (HR) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KDoF6b9ZLAI"
   },
   "source": [
    "In the last few years, the power of stronger processors, combined with new techniques for reducing size and cost of machine learning models (a process called quantization) has allowed ai to come all the way down to the internet of things (IoT) level. This is the level of microcontrollers and sensor systems. In other words, with recent proper techniques we can now deploy machine learning on really tiny devices that require very little power, such as watches, security cameras, and wearable medical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybR1g73pbj75"
   },
   "source": [
    "There are many benefits to being able to employ machine learning on IoT devices, and the possibilities are endless. In this tutorial, however, we will be taking a look at deploying a *task specific autoencoder*. Autoencoders are a type of neural network that, given an input, will reduce or compress the size to be very small. Then, they attempt to learn from this compressed signal by trying to recreate it as the output. This allows the model to learn what signals/features are key and should be preserved, and what should be ignored. In our case, we will be using this autoenocoder for feature-preserving compression. That is, we will encode data coming from a ppg sensor and use this to send over Bluetooth Low Energy (BLE) so that this data can be further analyzed by higher caliber and deeper networks with things such as Smooth Heart Rate prediction (HR). BLE is very low-power, and as such, cannot transmit large quanities of data easily such as a full ppg signal. As a result of this, the Autoencoder allows the data to be compressed to such small sizes that only the features needed for learning are kept, and this is a small enough size to be sent over through BLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7m5KuKdt3h"
   },
   "source": [
    "Let us start working on the project itself. You will need:\n",
    "\n",
    "\n",
    "1.   An OSU MotionSenseHRV chip\n",
    "2.   Anaconda or Miniconda installed (See https://docs.anaconda.com/anaconda/install/)\n",
    "\n",
    "After step 2, you'll have conda package manager (for python packages) either through your terminal (Linux, macOS) or through Anaconda Prompt (Windows). Open it in the \"tutorials\" folder as admin/sudo and run the following conda command. These will install all python dependencies in an environment named tf_MSHRV3_AEHR and activate it. Then, re-open this notebook from inside tf_MSHRV3_AEHR environment by simply executing \"jupyter notebook AE_model_tutorial.ipynb\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvCAiyOVPc_v"
   },
   "source": [
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate tf_MSHRV3_AEHR\n",
    "jupyter notebook AE_model_tutorial.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to Download the Data via. an automatic python script. Run this only once.\n",
    "\n",
    "#from data import get_data\n",
    "#get_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjMv94yRWD2T"
   },
   "source": [
    "**Make sure parameters and sample data are in the correct folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcPP2NEjVS17"
   },
   "source": [
    "We will start by getting the necessary model parameters and sample data in order to first test the model. To do this, run the get_data.py file in the repository, or run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zZQ83gIGcl0"
   },
   "outputs": [],
   "source": [
    "# TensorFlow is an open source machine learning library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras is TensorFlow's high-level API for deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Numpy is a math library\n",
    "import numpy as np\n",
    "# Pandas is a data manipulation library \n",
    "import pandas as pd\n",
    "# Scipy is a signal-processing library\n",
    "from scipy.signal import detrend\n",
    "# Matplotlib is a graphing library\n",
    "import matplotlib.pyplot as plt\n",
    "# Math is Python's math library\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#import some custom utility functions\n",
    "from utils import make_data_pipe, Rpeak2HR, sliding_window_fragmentation\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "path_prefix='./data/pre-training/'\n",
    "weights_path_prefix='./data/post-training/model_weights' #for saved weights\n",
    "\n",
    "val_files=[path_prefix+'2019092801_3154_clean.csv']\n",
    "test_files=[path_prefix+'2019092820_5701_clean.csv']\n",
    "win_len=8 #the window length, in seconds\n",
    "step=1 #in n_samples\n",
    "Fs_pks=100 #in Hz\n",
    "\n",
    "\n",
    "#TODO: this code will not run here for obvious reasons. In the public repository, will these files be located in the directory?\n",
    "#If so I will provide a paragraph to make sure it is in the right settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've configured our paths and imports, we should start by first loading our training data for the model, located in the repository as a spreadsheet csv file of a sample recorded ppg signal through time. We do this by using pd.readcsv(), whitch gets our data, and then we reformat and combine using numpy concatenations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uYZnMSSXlkB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_data(path,val_files=[],test_files=[],\n",
    "                   win_len=8,step=1,Fs_pks=100):\n",
    "\n",
    "    def get_clean_ppg_and_ecg(files):\n",
    "        list_clean_ppg=[];list_arr_pks=[]\n",
    "        for i in range(len(files)):\n",
    "            df=pd.read_csv(files[i],header=None)\n",
    "            arr=df.values\n",
    "            if 'clean' in files[i]:\n",
    "                arr[:,41:45]=(detrend(arr[:,41:45].reshape(-1),0,'constant')\n",
    "                                ).reshape((-1,4))\n",
    "                list_clean_ppg+=[np.concatenate([arr[:,29:30],arr[:,41:45]],\n",
    "                                    axis=-1),arr[:,30:31],arr[:,39:40],\n",
    "                                arr[:,40:41]]\n",
    "                list_arr_pks+=[arr[:,45:49].reshape(-1)]\n",
    "        return list_clean_ppg,list_arr_pks\n",
    "    files=glob.glob(path+'*.csv')\n",
    "    #files=[fil for fil in files if 'WZ' in fil] #get wenxiao's data\n",
    "    #separate val and test files\n",
    "    s3=set(files);s4=set(val_files+test_files)\n",
    "    files_2=list(s3.difference(s4))\n",
    "    #files_2=[files_2[0]]\n",
    "    #files_2=[fil for fil in files if not((val_names[0] in fil))]\n",
    "    list_clean_ppg,list_arr_pks=get_clean_ppg_and_ecg(files_2)\n",
    "\n",
    "    dsample_factr=4\n",
    "    Fs_pks=int(Fs_pks/dsample_factr)\n",
    "    win_len=win_len*Fs_pks\n",
    "\n",
    "    list_r_pk_locs=[np.arange(len(arr_pks))[arr_pks.astype(bool)] for\n",
    "                    arr_pks in list_arr_pks]\n",
    "\n",
    "    #get nearest dsampled idx\n",
    "    list_r_pk_locs_dsampled=[np.round(r_pk_locs/dsample_factr).astype(int) for\n",
    "                             r_pk_locs in list_r_pk_locs]\n",
    "\n",
    "    list_arr_pks_dsampled=[]\n",
    "    for j in range(len(list_arr_pks)):\n",
    "        arr_pks_dsampled=np.zeros([len(list_clean_ppg[4*j]),1])\n",
    "        #check & correct for rare rounding up issue in the last element\n",
    "        if list_r_pk_locs_dsampled[j][-1]==len(arr_pks_dsampled):\n",
    "            list_r_pk_locs_dsampled[j][-1]-=1\n",
    "        arr_pks_dsampled[list_r_pk_locs_dsampled[j]]=1\n",
    "        list_arr_pks_dsampled.append(arr_pks_dsampled)\n",
    "    #print([len(ppg) for ppg in list_arr_pks_dsampled])\n",
    "\n",
    "\n",
    "    list_HR=[dsample_factr*[Rpeak2HR(arr_pks,win_len,step,Fs_pks)]\n",
    "             for arr_pks in list_arr_pks_dsampled]\n",
    "    list_HR=sum(list_HR,[])\n",
    "    #list_HR=[HR[::dsample_factr] for HR in list_HR]\n",
    "\n",
    "    return list_clean_ppg,list_HR\n",
    "\n",
    "\n",
    "\n",
    "#input_list,output_list=[],[]\n",
    "list_sigs,list_HR=get_train_data(path_prefix,val_files,test_files,win_len,\n",
    "                                           step,Fs_pks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**import training data from csv file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FA0yDscf9ek"
   },
   "source": [
    "**Convert and Visualize Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPaS6RLufYqH"
   },
   "source": [
    "We will feed the data to the network using tensorflow's [tf.data](https://www.tensorflow.org/guide/data) pipeline which comes with lots of benefits (check out the link to learn more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx6Qza8Vf1jG"
   },
   "outputs": [],
   "source": [
    "#Pre-process data\n",
    "Fs_ppg=25 #Hz\n",
    "sample_win_len,step_size=win_len*Fs_ppg,2*Fs_ppg\n",
    "HR_win_len=sample_win_len\n",
    "ppg_win_len=sample_win_len+HR_win_len\n",
    "model_sigs_in,model_HR_out=[],[]\n",
    "for j in range(len(list_HR)):\n",
    "    #fragmenting our signal into 8 second windows\n",
    "    ppg,HR=list_sigs[j][:,0:1],list_HR[j]\n",
    "    ppg=sliding_window_fragmentation([ppg],ppg_win_len,step_size)\n",
    "    HR=sliding_window_fragmentation([HR],HR_win_len,step_size)\n",
    "    #chaining it all together\n",
    "    model_sigs_in.append(ppg)\n",
    "    model_HR_out.append(HR[:len(ppg)])\n",
    "\n",
    "#flattening the data to it is in a straight line\n",
    "model_sigs_in=np.concatenate(model_sigs_in,axis=0)\n",
    "model_HR_out=np.concatenate(model_HR_out,axis=0)\n",
    "model_in=model_sigs_in[:,:,0] \n",
    "model_out=model_HR_out[:,:,0]\n",
    "print(model_in.shape,model_out.shape)\n",
    "\n",
    "#Visualize our PPG signal\n",
    "idx=1\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.title('A sample PPG and HR')\n",
    "plt.plot(model_in[idx,:])\n",
    "plt.ylabel('PPG')\n",
    "plt.grid(True)\n",
    "plt.subplot(212)\n",
    "plt.plot(model_out[idx,:])\n",
    "plt.ylabel('HR (BPS)')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Sample No.')\n",
    "#partition\n",
    "val_perc=0.14\n",
    "val_idx=int(val_perc*len(model_in))\n",
    "\n",
    "#get our final formats and make the data pipe for our model\n",
    "val_data_AE=[model_in[0:val_idx,:],model_in[0:val_idx]]\n",
    "train_data_AE=[model_in[val_idx:,:],model_in[val_idx:]]\n",
    "val_data_e2e=[model_in[0:val_idx,:],model_out[0:val_idx]]\n",
    "train_data_e2e=[model_in[val_idx:,:],model_out[val_idx:]]\n",
    "\n",
    "train_ds_AE=make_data_pipe(train_data_AE,batch_size=32,shuffle=True)\n",
    "val_ds_AE=make_data_pipe(val_data_AE,batch_size=128,shuffle=False)\n",
    "train_ds_e2e=make_data_pipe(train_data_e2e,batch_size=32,shuffle=True)\n",
    "val_ds_e2e=make_data_pipe(val_data_e2e,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fWoLh6FGbYl"
   },
   "source": [
    "**Loading/Training the model**\n",
    "\n",
    "Now that we have the necessary data in order to properly load, train and test our model, we will start by importing it and then pre processing it.\n",
    "\n",
    "We begin by constructing our machine learning model. Our target network is an AutoEncoder that will encode 8 seconds of a ppg signal into a single 1x16 latent representation (or \"code\"). The layers in between are responsible for the feature-selective compression. If you look at the model, we start with our input, then  convert it into a 256 x 1 layer, with a rectified linear activation function. This is then shrunk to a 64 layer, and then finally our 16 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5lnHXtYlzgm"
   },
   "outputs": [],
   "source": [
    "def create_model_AE(in_shape,latent_shape=(16,),encoder=None,decoder=None):\n",
    "    #We will be creating the AE_model here\n",
    "    if encoder is None:\n",
    "        encoder_in = layers.Input(shape=in_shape,name=\"encoder_in\")\n",
    "        x = layers.Dense(256,activation='relu')(encoder_in)\n",
    "        x = layers.Dense(64,activation='relu')(x)\n",
    "        encoder_out = layers.Dense(latent_shape[0],activation='relu')(x)\n",
    "        encoder = keras.Model(encoder_in, encoder_out, name=\"encoder\")\n",
    "\n",
    "    #our encoder is what will be on the edge-device. Since we also need to uncompress,\n",
    "    #or decode it, we must also create a decoder, and then combine them to together in order to train them\n",
    "    if decoder is None:\n",
    "        decoder_in = layers.Input(shape=latent_shape,name=\"decoder_in\")\n",
    "        x = layers.Dense(64,activation='relu')(decoder_in)\n",
    "        x = layers.Dense(256,activation='relu')(x)\n",
    "        decoder_out = layers.Dense(in_shape[0])(x)\n",
    "        decoder = keras.Model(decoder_in, decoder_out, name=\"decoder\")\n",
    "    \n",
    "    AE_in = keras.Input(shape=in_shape, name=\"AE_in\")\n",
    "    z = encoder(AE_in)\n",
    "    sig_hat = decoder(z)\n",
    "    model_AE = keras.Model(AE_in, sig_hat, name=\"AE\")\n",
    "    return encoder,decoder,model_AE\n",
    "\n",
    "#now we will use this function to make the AE model\n",
    "encoder,decoder,model_AE = create_model_AE(model_in.shape[1:],\n",
    "                                           latent_shape=(16,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have our auto encoder, let's also connect our smooth heart rate prediction model to the autoencoder to that it can be optimized to be task specific. the HR model will append onto the decoder and find the heart rate from the reconstructed ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing PPG to HR Neural Network model called model_HR for smooth Heart Rate Prediction from one channel PPG signal\n",
    "def create_model_HR(in_shape,HR_win_len=200):\n",
    "    expand_dims = layers.Lambda(lambda x: tf.expand_dims(x,axis=-1), \n",
    "                                name='expand_dims')\n",
    "    #RNN model via. Functional API\n",
    "    rnn = layers.GRU(64, return_sequences=True, return_state=True)\n",
    "    sig_in = layers.Input(shape=in_shape)\n",
    "    x = expand_dims(sig_in)\n",
    "    _, final_state=rnn(x[:,:HR_win_len,:]) #warm-up RNN\n",
    "    rnn_out, _ = rnn(x[:,HR_win_len:,:],initial_state=final_state)\n",
    "    HR_hat=layers.Conv1D(filters=1,kernel_size=1, strides=1,padding='same',\n",
    "                         activation=None,name='Conv_{}'.format(1))(rnn_out)\n",
    "    HR_hat=layers.Flatten()(HR_hat)\n",
    "    model = keras.Model(sig_in, HR_hat, name='model_HR')\n",
    "    return model\n",
    "\n",
    "#% Load HR model\n",
    "model_HR = create_model_HR(model_in.shape[1:],HR_win_len)\n",
    "weights_dir_HR=weights_path_prefix+'/sig2HR/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_HR)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_HR.load_weights(latest_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We then stack the model_AE and model_HR together in a model called model_e2e (end2end).\n",
    "def create_model_e2e(in_shape,encoder,decoder,model_HR):\n",
    "    #Put the models together\n",
    "    e2e_in = keras.Input(shape=in_shape, name=\"e2e_in\")\n",
    "    HR_hat=model_HR(decoder(encoder(e2e_in)))\n",
    "    model_e2e = keras.Model(e2e_in, HR_hat, name=\"e2e\")\n",
    "    return model_e2e\n",
    "\n",
    "# Make e2e model\n",
    "model_e2e = create_model_e2e(model_in.shape[1:],encoder,decoder,model_HR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's inspect our model and, finally, train it. The training process may take a while depending on your PC hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model_in.shape is the input format\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(model_AE.summary())\n",
    "print(model_HR.summary())\n",
    "print(model_e2e.summary())\n",
    "\n",
    "plot_model=False\n",
    "file_path='./data/figures'\n",
    "os.makedirs(file_path,exist_ok=True)\n",
    "\n",
    "if plot_model:\n",
    "    tf.keras.utils.plot_model(model_AE,to_file=file_path+'/AE.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "    tf.keras.utils.plot_model(model_e2e,to_file=file_path+'/e2e.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#compiling the model prepares it for training\n",
    "model_AE.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "#we will also set a directory to save our trained weights once they are finished\n",
    "weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "os.makedirs(weights_dir_AE,exist_ok=True)\n",
    "ckpt_filepath_AE=weights_dir_AE+'/cp.ckpt'\n",
    "#callbacks\n",
    "callbacks=[]\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_AE,\n",
    "                    save_weights_only=True,save_best_only=True,\n",
    "                    monitor=\"val_loss\",mode='min'))\n",
    "\n",
    "# Train Simple AE for reconstruction\n",
    "model_AE.fit(train_ds_AE,epochs=400,validation_data=val_ds_AE,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best AE ckpt\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_AE)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "\n",
    "# Train End2End model\n",
    "model_e2e.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "weights_dir_e2e=weights_path_prefix+'/AE/checkpoints_e2e'\n",
    "os.makedirs(weights_dir_e2e,exist_ok=True)\n",
    "ckpt_filepath_e2e=weights_dir_e2e+'/cp.ckpt'\n",
    "\n",
    "#callbacks\n",
    "callbacks=[]\n",
    "\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e,\n",
    "                save_weights_only=True,save_best_only=True,\n",
    "                monitor=\"val_loss\",mode='min'))\n",
    "\n",
    "# Train\n",
    "model_e2e.fit(train_ds_e2e,epochs=100,validation_data=val_ds_e2e,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTx0MUtQnGC1"
   },
   "source": [
    "**Prepare for Quanitization aware training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXBQMqBMnNuc"
   },
   "source": [
    "Now that we have our model ready, we should now use something called quantization aware training (QAT), that will train our network in preparation to be converted to a lighter format. Integer Quantization is the process of converting parameters (weights, bias, and activations) in our network to 8 bit integer values, used because 8bit integer values are both faster and lighter in terms of storage, whitch is very helpful for microcontrollers. While QAT is not the quantization process itself, This training will optimize our weights to the 8bit model and fine tune them to prepare for this conversion process. It does this by simulating what the model would run like if it used integers, and then optimizing these pre-converted numbers to minized the loss function AFTER it has been trained. Note that we have not actually converted the numbers by involking QAT, we have just prepared the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba-PeGYWn0J5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmopt\n",
    "\n",
    "#Load best e2e ckpt\n",
    "#latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e)\n",
    "#print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "#model_e2e.load_weights(latest_ckpt)\n",
    "\n",
    "#performing generation of the quantization aware model of the encoder\n",
    "quan_model_generator = tfmopt.quantization.keras.quantize_model\n",
    "encoder_q = quan_model_generator(encoder) #quantization aware encoder\n",
    "decoder_noq = keras.models.clone_model(decoder)\n",
    "decoder_noq.set_weights(decoder.get_weights())\n",
    "decoder_noq.trainable=False #Freeze decoder for now\n",
    "    \n",
    "# Make the AE model, for quantization of it later on\n",
    "_,_,model_AE_q = create_model_AE(model_in.shape[1:],\n",
    "latent_shape=(16,),encoder=encoder_q,decoder=decoder_noq)\n",
    "# Make e2e model\n",
    "model_e2e_q = create_model_e2e(model_in.shape[1:],encoder_q,decoder_noq,model_HR)\n",
    "\n",
    "\n",
    "#model_in.shape is the input format, you should see (0, 16)\n",
    "print(encoder_q.summary())\n",
    "\n",
    "if plot_model:\n",
    "    tf.keras.utils.plot_model(model_e2e_q,to_file=file_path+'/e2e_q.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Train QAT model**\n",
    "\n",
    "The QAT version of our model has now been configured properly and is ready to 8-bit optimize, by training. We train this model in the same fashion that we trained our original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_e2e_q.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "#we set the directory to our pretrained path\n",
    "weights_dir_e2e_q = weights_path_prefix+'/AE/checkpoints_e2e_q'\n",
    "os.makedirs(weights_dir_e2e_q,exist_ok=True)\n",
    "ckpt_filepath_e2e_q=weights_dir_e2e_q+'/cp.ckpt'\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e_q,\n",
    "                save_weights_only=True,save_best_only=True,\n",
    "                monitor=\"val_loss\",mode='min'))\n",
    "# Train\n",
    "model_e2e_q.fit(train_ds_e2e,epochs=100,validation_data=val_ds_e2e,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgqBJSCzoryb"
   },
   "source": [
    "**Compare QAT results with normal model**\n",
    "We also should make sure that most of our accuracy is preserved during this conversion process. Here we check only using validation set but we encourage and leave this verification, using the test data, as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryFFx7HAorCN"
   },
   "outputs": [],
   "source": [
    "#Load best e2e_q checkpoint, training by the model\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e_q)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_e2e_q.load_weights(latest_ckpt)\n",
    "\n",
    "baseline_model_mse = model_e2e.evaluate(val_ds_e2e, verbose=1)\n",
    "q_aware_model_mse = model_e2e_q.evaluate(val_ds_e2e, verbose=1)\n",
    "\n",
    "print('Baseline val mse:', baseline_model_mse)\n",
    "print('Quant val mse:', q_aware_model_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APir38KsqMIN"
   },
   "source": [
    "**Quantize the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc_1HxR6qTbT"
   },
   "source": [
    "We are now fully ready to convert the model into it's lightweight component, an 8 bit fixed point version of our model. Normally, in machine learning models we use a floating point 32 type of number, whitch is a number that supports decimal points and has a high precision amount, meaning you can type a lot of numbers into it. 8 bit integers only support up to 256 possible numbers, and as such, are not as accurate, but are much faster to run. After running this code, a model named according to the string in \"filename\". Quantization is typically done by representing the number by this format: Real number = most_significant_digits_of_real_number*scaling_factor. This is the esense of what the converter is doing, though there are many optimizations you can make to ensure this runs with optimal accuracy. You can do this by modifying the attributes of the converter objects, as you can see in the code below. However, since we have employed Quantization Awareness training, most of these settings are automated, and we just need to make sure only one flag, converter.optimizations, is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw0wwQXdq3oP"
   },
   "outputs": [],
   "source": [
    "file_name = \"tf_lit2_encoder.tflite\"\n",
    "#prepare to convert the encoder model for deeployment by getting our converter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(encoder_q)\n",
    "\n",
    "\n",
    "#the following optimizations are already configured by QAT:\n",
    "#converter.representitive_dataset\n",
    "#converter.target_spec\n",
    "#converter.inference_input_type and converter.inference_output_type\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "#this converts the model to it's 8 bit weight, bias, and activation form\n",
    "encoder_tf_lite = converter.convert()\n",
    "\n",
    "#we now have our model, and we can save it to our directory for deployment\n",
    "open(file_name, \"wb\").write(encoder_tf_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjHfb6pfrFHQ"
   },
   "source": [
    "**Compare our model to the original**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA8oY_fhthJ8"
   },
   "source": [
    "To make sure our model works, we will now test it with the baseline model and compare mean-sqaure error values to measure how accurate they are. In Tensorflow, you can to this with model.evaluate, but because we are now also working with a tensorflow lite model format we need to pass induvidual values to it through a loop and then stack the values with numpy. Autoencoders work just like any other network in that you can compare the results of the network's output with the desired result, in this case our real ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwh43oVM6PkC"
   },
   "outputs": [],
   "source": [
    "test_tf_lite_model = tf.lite.Interpreter(\"tf_lit2_encoder.tflite\")\n",
    "#you can change the name of this model in the program by renaming the \"file_name\" variable and then\n",
    "#replaceing the above string with your file_name variable\n",
    "test_tf_lite_model.allocate_tensors()\n",
    "#the model involking actually works the same way in tflite micro, the only difference is the language\n",
    "input_tensor = test_tf_lite_model.get_input_details()[0][\"index\"]\n",
    "output_tensor = test_tf_lite_model.get_output_details()\n",
    "\n",
    "repre_data_set = np.array(val_data_AE[1])\n",
    "np.random.shuffle(repre_data_set)\n",
    "\n",
    "result_difference = 0\n",
    "results = []\n",
    "sample_test_num = 3493\n",
    "first_time = False\n",
    "# Run the model's interpreter for each value and store the results in arrays\n",
    "for i in range(0, sample_test_num):\n",
    "    calibrat_data = repre_data_set[i]\n",
    "    calibrat_data = np.reshape(calibrat_data, (1, 400))\n",
    "    tensor_calibrat = tf.convert_to_tensor(calibrat_data, dtype=tf.float32)\n",
    "    test_tf_lite_model.set_tensor(input_tensor, tensor_calibrat)\n",
    "    test_tf_lite_model.invoke()\n",
    "    output_data = test_tf_lite_model.get_tensor(output_tensor[0]['index'])\n",
    "    normal_model_output = 0\n",
    "    if not first_time:\n",
    "        result = np.array(output_data)\n",
    "        first_time = True\n",
    "        result_parent = np.array(normal_model_output)\n",
    "    else:\n",
    "        result = np.vstack((result, output_data))\n",
    "        result_parent = np.vstack((result_parent, normal_model_output))\n",
    "\n",
    "    #now we have the data from both sets encoder models. All we need to do is pass them through data\n",
    "\n",
    "    #now we are just taking the average of the difference of each value in the output tensor and printing this\n",
    "    result_arr = np.subtract(normal_model_output, output_data)\n",
    "    result_arr = np.absolute(result_arr)\n",
    "    result_difference = np.mean(result_arr)\n",
    "    #we are taking a mean here to compare the result_difference)\n",
    "    results.append(float(result_difference))\n",
    "\n",
    "final_diff = np.mean(results)\n",
    "result_parent = encoder.predict(repre_data_set)\n",
    "decoder.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "print(\"the mean ouput loss across\", sample_test_num, \"runs is \", final_diff)\n",
    "tflite_loss = decoder.evaluate(result, repre_data_set)\n",
    "parent_loss = decoder.evaluate(result_parent, repre_data_set)\n",
    "model_AE.evaluate(repre_data_set, repre_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64zKtXlvMHWY"
   },
   "source": [
    "**Convert the model to a Tflite micro model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjlVHTr5NKtu"
   },
   "source": [
    "Now that we have our full tensorflow lite model, we'll need to convert this model into a tf-lite-micro file so that it can be deployed to a micro-controller. You can do this by opening up Linux and running the following commands in the terminal, in the same directory as your tflite file. Make sure you replace the {MODEL_TFLITE} and {MODEL_TFLITE_MICRO} with the tflite file name and the converted name you want the file to be, respectively. Note that we will be continuing the rest of this tutorial in linux. For Windows users, xxd package is available in Vim for Windows, though this has not been tested to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install xxd if it is not available\n",
    "!apt-get update && apt-get -qq install xxd\n",
    "\n",
    "### Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Apb78TZSZIG"
   },
   "source": [
    "### **Load a tensorflow nrf-project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a--Qwq0XS6VO"
   },
   "source": [
    "This is where our project gets a little specific. So far we currently have one tutorial avalible with an NRF5340 Preveiw Developement kit (PDK). We are currently working on the  MotionSense_v3 Device specific instructions right now and will update this tutorial soon. So Stay Tuned!!..."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Deploying to NRF 5340 Preview Development Kit**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section of the tutorial is designed to work with Nordic Semiconductor's NRF5340 Preview Development kit. You can obtain one for yourself by buying one at https://www.nordicsemi.com/Software-and-tools/Development-Kits/nRF5340-PDK, if you don't already have one.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Download the NRF-Connect SDK**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we get started, you'll need to first download all the software in order to program and debug the microcontroller with ease. You can get started by visiting this link (http://developer.nordicsemi.com/nRF_Connect_SDK/doc/latest/nrf/gs_installing.html) and following the instructions specified exactly in *Installing the nRF Connect SDK manually* When you get to the part that says \"*Cloning the repositories*\" make sure you install the nrf connect sdk version 1.4.0. To do this, you should run the following (following the simalar syntax of the tutorial):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!west init -m https://github.com/nrfconnect/sdk-nrf --mr v1.4.0\n",
    "!west update\n",
    "!west zephyr-export"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you also install the additional python dependencies found under *\"Installing additional Python dependencies*\" section. Finally, you should get the Nordic version of Segger Embedded Studio, nordiac version, as per the NRF-connect manual setup intructions under \"Installing SEGGER Embedded Studio Nordic Edition\" and \"Setting up the SES environment\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###**Configure the Tensorflow Lite Micro Library**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've got the nrf-connect sdk, it's time to install the Tensorflow Lite Micro Library. First, you should start by cloning the TensorFlow repo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/tensorflow/tensorflow.git\n",
    "!make -f tensorflow/lite/micro/tools/make/Makefile third_party_downloads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% mc\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Update the model with the newly created version**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the project has opened, you should see a file named model.cc. This is a template model that the application is using, and we will need to replace this with our own model. In the project directory, go to the CmakeLists.txt file and open it. You should see something like the following:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "cmake_minimum_required(VERSION 3.13.1)\n",
    "find_package(Zephyr HINTS $ENV{ZEPHYR_BASE})\n",
    "project(external_lib)\n",
    "\n",
    "#target_sources(app PRIVATE src/hello_world_test.cc)\n",
    "target_sources(app PRIVATE src/main.cc)\n",
    "target_sources(app PRIVATE src/main_functions.cc)\n",
    "target_sources(app PRIVATE src/constants.cc)\n",
    "target_sources(app PRIVATE src/output_handler.cc)\n",
    "#replace the model with your model file here\n",
    "target_sources(app PRIVATE src/your_model_name.cc)\n",
    "target_sources(app PRIVATE src/assert.cc)\n",
    "target_sources(app PRIVATE src/tensorFlowService.cc)\n",
    "target_sources(app PRIVATE src/aencoder.cc)\n",
    "\n",
    "zephyr_include_directories(src)\n",
    "\n",
    "# The external static library that we are linking with does not know\n",
    "# how to build for this platform so we export all the flags used in\n",
    "# this zephyr build to the external build system.\n",
    "#\n",
    "# Other external build systems may be self-contained enough that they\n",
    "# do not need any build information from zephyr. Or they may be\n",
    "# incompatible with certain zephyr options and need them to be\n",
    "# filtered out.\n",
    "\n",
    "set(TARGET ${BOARD})\n",
    "set(TARGET_ARCH cortex-m33)\n",
    "\n",
    "# you need to fix the following path before you run the code\n",
    "set(TF_SRC_DIR  your_location_of_the_tensorflow_repository\n",
    "set(TF_MAKE_DIR ${TF_SRC_DIR}/tensorflow/lite/micro/tools/make)\n",
    "#you can set the TF_LIB_DIR path as this one, or you can also set it to whatever folder your library folder is in.\n",
    "set(TF_LIB_DIR ${TF_MAKE_DIR}/gen/${TARGET}_${TARGET_ARCH}/lib)\n",
    "set(extra_project_flags \"-mcpu=${TARGET_ARCH} -mthumb -mno-thumb-interwork -mhard-float -mfpu=fpv5-sp-d16\") #I had to remove -DTF_LITE_STATIC_MEMORY to make hello_world run.\n",
    "\n",
    "zephyr_get_include_directories_for_lang_as_string(       C C_includes)\n",
    "zephyr_get_system_include_directories_for_lang_as_string(C C_system_includes)\n",
    "zephyr_get_compile_definitions_for_lang_as_string(       C C_definitions)\n",
    "zephyr_get_compile_options_for_lang_as_string(           C C_options)\n",
    "\n",
    "set(external_project_cflags\n",
    "  \"${C_includes} ${C_definitions} ${optC_optionsions} ${C_system_includes} ${extra_project_flags}\"\n",
    ")\n",
    "\n",
    "zephyr_get_include_directories_for_lang_as_string(       CXX CXX_includes)\n",
    "zephyr_get_system_include_directories_for_lang_as_string(CXX CXX_system_includes)\n",
    "zephyr_get_compile_definitions_for_lang_as_string(       CXX CXX_definitions)\n",
    "zephyr_get_compile_options_for_lang_as_string(           CXX CXX_options)\n",
    "\n",
    "set(external_project_cxxflags\n",
    "  \"${CXX_includes} ${CXX_definitions} ${CXX_options} ${CXX_system_includes} ${extra_project_flags}\"\n",
    ")\n",
    "\n",
    "include(ExternalProject)\n",
    "\n",
    "\n",
    "#here is where we include the tf micro library. Make sure the tensorflow .a library is located\n",
    "#where the TF_LIB_DIR is set\n",
    "\n",
    "\n",
    "if(CMAKE_GENERATOR STREQUAL \"Unix Makefiles\")\n",
    "# https://www.gnu.org/software/make/manual/html_node/MAKE-Variable.html\n",
    "set(submake \"$(MAKE)\")\n",
    "else() # Obviously no MAKEFLAGS. Let's hope a \"make\" can be found somewhere.\n",
    "set(submake \"make\")\n",
    "endif()\n",
    "\n",
    "ExternalProject_Add(\n",
    "  tf_project                 # Name for custom target\n",
    "  #PREFIX     ${mylib_build_dir} # Root dir for entire project\n",
    "  SOURCE_DIR ${TF_SRC_DIR}\n",
    "  BINARY_DIR ${TF_SRC_DIR} # This particular build system is invoked from the root\n",
    "  CONFIGURE_COMMAND \"\"    # Skip configuring the project, e.g. with autoconf\n",
    "  BUILD_COMMAND\n",
    "  ${submake} -f tensorflow/lite/micro/tools/make/Makefile\n",
    "  TARGET=${TARGET}\n",
    "  TARGET_ARCH=${TARGET_ARCH}\n",
    "  TARGET_TOOLCHAIN_ROOT=${GNUARMEMB_TOOLCHAIN_PATH}/bin/\n",
    "  TARGET_TOOLCHAIN_PREFIX=arm-none-eabi-\n",
    "  #PREFIX=${mylib_build_dir}\n",
    "  CC=${CMAKE_C_COMPILER}\n",
    "  CXX=${CMAKE_CXX_COMPILER}\n",
    "  AR=${CMAKE_AR}\n",
    "  CCFLAGS=${external_project_cflags}\n",
    "  CXXFLAGS=${external_project_cxxflags}\n",
    "  microlite\n",
    "  INSTALL_COMMAND \"\"      # This particular build system has no install command\n",
    "  BUILD_BYPRODUCTS ${TF_LIB_DIR}/libtensorflow-microlite.a\n",
    "  )\n",
    "\n",
    "# Create a wrapper CMake library that our app can link with\n",
    "add_library(tf_lib STATIC IMPORTED GLOBAL)\n",
    "\n",
    "add_dependencies(\n",
    "  tf_lib\n",
    "  tf_project\n",
    "  )\n",
    "set_target_properties(tf_lib PROPERTIES IMPORTED_LOCATION             ${TF_LIB_DIR}/libtensorflow-microlite.a)\n",
    "set_target_properties(tf_lib PROPERTIES INTERFACE_INCLUDE_DIRECTORIES \"${TF_SRC_DIR};${TF_SRC_DIR}/tensorflow/lite/micro;${TF_MAKE_DIR}/downloads/flatbuffers/include\")\n",
    "\n",
    "target_link_libraries(app PUBLIC tf_lib)\n",
    "\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To link your own model, replace the file in the code marked\n",
    "`target_sources(app PRIVATE src/your_model_name.cc)`\n",
    "with your own generated file from above.\n",
    "Then, you also need to follow the intructions in the comments: First make sure the TF_SRC_DIR is set to your cloned tensorflow repository (i.e. home/user_name/documents/tensorflow) and that the TF_LIB_DIR is set to the precompiled library (libtensorflow-microlite.a)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Inspect the code for the tensorflow lite micro model startup**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside the Embedded studio Project you will find a file labeled main_functions.cc. When you run run the project on your device, this is the main stack where most of the tensorflow inference is done. As we are using a PPG sensor, and the NRF5340 does not have an onboard version of this, we use a subset of the same testing data when we initally trained the network. This is located in sample_test.cc. Note that if you constructed your own model, you must use your own samples/sensors, and also structure your own for loop to place data in the input tensors as per the dimentionality of your input (this size can be found by looking at TfLiteIntArray* n_dims = input->dims). The data can be fed into the model by placeing it in sequential (left to right, up to down, 3rd tensor_dim...etc) order in a contiguous format in input->data.f."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```cpp\n",
    "\n",
    "#include \"main_functions.h\"\n",
    "#include \"sample_test.h\"\n",
    "\n",
    "#define USENEWMODEL 1\n",
    "\n",
    "#include \"tensorflow/lite/micro/all_ops_resolver.h\"\n",
    "#include \"constants.h\"\n",
    "#include \"output_handler.h\"\n",
    "#include \"tensorflow/lite/micro/micro_error_reporter.h\"\n",
    "#include \"tensorflow/lite/micro/micro_interpreter.h\"\n",
    "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
    "#include \"tensorflow/lite/version.h\"\n",
    "\n",
    "\n",
    "#if USENEWMODEL == 1\n",
    "#include \"your_model_name.cc\"\n",
    "#define MODEL_LEN //length of your model, defined as int g_modelurd_length in your_model_name.cc\n",
    "#define MODEL_HEADROOM //make this number at least > 2000 if using an autoencoder\n",
    "#define g_modelurd2 0\n",
    "\n",
    "#else\n",
    "#include \"aencoder.h\"\n",
    "#define MODEL_LEN 123904\n",
    "#define MODEL_HEADROOM 200000\n",
    "#define g_modelurd 0\n",
    "#endif\n",
    "\n",
    "\n",
    "#include <zephyr.h>\n",
    "\n",
    "// Globals, used for compatibility with Arduino-style sketches.\n",
    "namespace\n",
    "{\n",
    "tflite::ErrorReporter *error_reporter = nullptr;\n",
    "const tflite::Model *model = nullptr;\n",
    "tflite::MicroInterpreter *interpreter = nullptr;\n",
    "TfLiteTensor *input = nullptr;\n",
    "TfLiteTensor *output = nullptr;\n",
    "int inference_count = 0;\n",
    "\n",
    "// Create an area of memory to use for input, output, and intermediate arrays.\n",
    "// Minimum arena size, at the time of writing. After allocating tensors\n",
    "// you can retrieve this value by invoking interpreter.arena_used_bytes().\n",
    "const int kModelArenaSize = MODEL_LEN;\n",
    "// Extra headroom for model + alignment + future interpreter changes.\n",
    "const int kExtraArenaSize = MODEL_HEADROOM;\n",
    "const int kTensorArenaSize = kModelArenaSize + kExtraArenaSize;\n",
    "static uint8_t tensor_arena[kTensorArenaSize] ;\n",
    "} // namespace\n",
    "\n",
    "// The name of this function is important for Arduino compatibility.\n",
    "void setup()\n",
    "{\n",
    "\toutputInit();\n",
    "\n",
    "\t// Set up logging. Google style is to avoid globals or statics because of\n",
    "\t// lifetime uncertainty, but since this has a trivial destructor it's okay.\n",
    "\t// NOLINTNEXTLINE(runtime-global-variables)\n",
    "\n",
    "\tstatic tflite::MicroErrorReporter micro_error_reporter;\n",
    "\terror_reporter = &micro_error_reporter;\n",
    "\n",
    "\t// Map the model into a usable data structure. This doesn't involve any\n",
    "\t// copying or parsing, it's a very lightweight operation.\n",
    "\t#if USENEWMODEL == 1\n",
    "            model = tflite::GetModel(g_modelurd2);\n",
    "\n",
    "        #else\n",
    "            model = tflite::GetModel(g_modelurd);\n",
    "        #endif\n",
    "\n",
    "\tif (model->version() != TFLITE_SCHEMA_VERSION) {\n",
    "\t\tTF_LITE_REPORT_ERROR(\n",
    "\t\t\terror_reporter,\n",
    "\t\t\t\"Model provided is schema version %d not equal \"\n",
    "\t\t\t\"to supported version %d.\",\n",
    "\t\t\tmodel->version(), TFLITE_SCHEMA_VERSION);\n",
    "\t\treturn;\n",
    "\t}\n",
    "\n",
    "\t// This pulls in all the operation implementations we need.\n",
    "\t// NOLINTNEXTLINE(runtime-global-variables)\n",
    "\tstatic tflite::AllOpsResolver resolver;\n",
    "\n",
    "\t// Build an interpreter to run the model with.\n",
    "\tstatic tflite::MicroInterpreter static_interpreter(model, resolver,\n",
    "\t\t\t\t\t\t\t   tensor_arena,\n",
    "\t\t\t\t\t\t\t   kTensorArenaSize,\n",
    "\t\t\t\t\t\t\t   error_reporter);\n",
    "\tinterpreter = &static_interpreter;\n",
    "\n",
    "\t// Allocate memory from the tensor_arena for the model's tensors.\n",
    "\tTfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
    "\n",
    "\tif (allocate_status != kTfLiteOk) {\n",
    "\n",
    "\t\treturn;\n",
    "\t}\n",
    "\n",
    "\t// Obtain pointers to the model's input and output tensors.\n",
    "\tinput = interpreter->input(0);\n",
    "\toutput = interpreter->output(0);\n",
    "\n",
    "\t// Keep track of how many inferences we have performed.\n",
    "\tinference_count = 0;\n",
    "}\n",
    "\n",
    "// The name of this function is important for Arduino compatibility.\n",
    "float * loop()\n",
    "{\n",
    "float y[16];\n",
    "uint32_t start_time;\n",
    "uint32_t stop_time;\n",
    "uint32_t cycles_spent;\n",
    "uint32_t nanoseconds_spent;\n",
    "\n",
    "/* capture initial time stamp */\n",
    "start_time = k_cycle_get_32();\n",
    "\n",
    "        //first set our output array\n",
    "        static float arr_result[16];\n",
    "\n",
    "\t//get a ppg sample to feed into the model. We this is an 8 second\n",
    "\t// sample for a total of 400 values, whitch should be placed in our input.\n",
    "\n",
    "\n",
    "        TfLiteIntArray* n_dims = input->dims;\n",
    "\t// Place the calculated ppg value in the model's input tensor\n",
    "\n",
    "        /*.f is not actually the floating point cast, it is a a structure member\n",
    "        since it is a union, each of the . data types represents any possible input format.\n",
    "        This could mean you could represent your input in uint8, or float32. We use\n",
    "        float32 since that is the default specification for inputs and outputs in\n",
    "        tensorflow lite models.\n",
    "        */\n",
    "\n",
    "\tfloat xval = 0;\n",
    "        //get ready to load the test ppg data into the input tensor\n",
    "        for (int i = 0; i<400; i++){\n",
    "                xval = sample_data[i];\n",
    "                input->data.f[i] = sample_data[i];\n",
    "        }\n",
    "        //just using this test variable to inspect whether everything checks out\n",
    "        float test_bounds = input->data.f[399];\n",
    "\n",
    "\n",
    "\n",
    "\t//Now we run inference, and report any error\n",
    "\tTfLiteStatus invoke_status = interpreter->Invoke();\n",
    "\tif (invoke_status != kTfLiteOk) {\n",
    "\t\tTF_LITE_REPORT_ERROR(error_reporter,\"Invoke failed\");\n",
    "\n",
    "\t\treturn NULL;\n",
    "\t}\n",
    "\n",
    "\t// Read the predicted y value from the model's output tensor\n",
    "\n",
    "\n",
    "\t// Output the results. A custom HandleOutput function can be implemented\n",
    "\t// for each supported hardware target.\n",
    "\n",
    "\n",
    "        for (int i = 0; i < 16; i++){\n",
    "            arr_result[i] = output->data.f[i];\n",
    "            }\n",
    "        y[0] = output->data.f[0];\n",
    "        y[1] = output->data.f[1];\n",
    "        y[2] = output->data.f[2];\n",
    "        y[3] = output->data.f[3];\n",
    "        y[4] = output->data.f[4];\n",
    "        y[5] = output->data.f[5];\n",
    "        y[6] = output->data.f[6];\n",
    "        y[7] = output->data.f[7];\n",
    "        y[8]  = output->data.f[8];\n",
    "        y[9] = output->data.f[9];\n",
    "        y[10] = output->data.f[10];\n",
    "        y[11] = output->data.f[11];\n",
    "        y[12] = output->data.f[12];\n",
    "        y[13] = output->data.f[13];\n",
    "        y[14] = output->data.f[14];\n",
    "        y[15] = output->data.f[15];\n",
    "\n",
    "\t// Increment the inference_counter, and reset it if we have reached\n",
    "\t// the total number per cycle\n",
    "\tinference_count += 1;\n",
    "        if (inference_count > 2000){\n",
    "            inference_count = 0;\n",
    "                       }\n",
    "        int te = inference_count;\n",
    "        stop_time = k_cycle_get_32();\n",
    "\n",
    "/* compute how long the work took (assumes no counter rollover) */\n",
    "cycles_spent = stop_time - start_time;\n",
    "nanoseconds_spent = SYS_CLOCK_HW_CYCLES_TO_NS(cycles_spent);\n",
    "        printk(\"time spent, %d\\n\", nanoseconds_spent);\n",
    "\treturn y;\n",
    "}\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at this code, we have the following. First, we setup all of our variables. The main objects we have are the error reporter and interpreter, whitch are static global objects stored as in our heap, so that it will be permanently in memory. The error reporter object attaches to the interpreter, and will evalute the interpreter closely to make sure it running inferences accuratly and not messing up or erroring. Then, our model object is created, whitch is actually just the model we imported. This model object is a serialized format of the entire model. It is composed of all the weights and bias from out parent model, as well as information about the layers and all of the activation functions, whitch is all encoded in a c byte array that you can see in your created tflite micro file. At runtime, the interpreter interprets these bytes and with the method AllocateTensors(), and divides the provided static array object into seperate little layers where each of the weights and bias are stored. Through this method, the entire the entire working model is stored. With this, we are ready to be able to use the model. The function Interpreter.Invoke() is a blocking call (since TF-lite is designed to run at bare-metal systems without multithreading or any sort of operating system, and evalutes each of the layers one at a time, making space in the same given static array for the results of each layer as it goes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Verify the dimentions for the tensorflow lite micro inputs and outputs**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "when you initially run it with Segger Embedded Studio, you should see 2 print statements. These are for verification that the model is working properly. The second statement prints out the input and output tensors dimentions. Verify that these are correct with the model that you imported. Also check that this matches the code to place in the input tensors in. The Snippet looks like this:\n",
    "\n",
    "```cpp\n",
    "//get ready to load the test ppg data into the input tensor\n",
    "        for (int i = 0; i<400; i++){\n",
    "                input->data.f[i] = sample_data[i];\n",
    "        }\n",
    "        float test_bounds = input->data.f[399];\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And should match the number in the input tensor print statement."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Receiving the output data on host device**\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you look at the bottom of the code, you'll see our model is returning the results as a float pointer. in the file tensorflow_service.cc and main.cc, there is code from both us, the Zephr OS, and the Nordic SoftDevice library that handles this data, puts it in bluetooth registers and then sends it out when the OS can schedule a task to do so. We pick up this ble signal by identifying our device's UUID signal and then reading the data in a format specified by GATT services. This can be done either with MATLAB, or inside python itself with the blupy library. The code to do this looks a little like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import gmtime, strftime, sleep\n",
    "from bluepy.btle import Scanner, DefaultDelegate, BTLEException\n",
    "import sys\n",
    "from bluepy.btle import UUID, Peripheral, ADDR_TYPE_RANDOM, DefaultDelegate\n",
    "import argparse\n",
    "import time\n",
    "import struct\n",
    "import binascii\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "from threading import Thread\n",
    "import subprocess\n",
    "\n",
    "\n",
    "'''\n",
    "For Adaptive setting:\n",
    "20 s calibration time. Have some margin from Saturation, ma.\n",
    "Hyperparams: ma, thres_max for green S/N, thres_min for infra/red DC value.\n",
    "Green (main Channel): Start Green from (maximum intensity-ma) and reduce it until S/N ratio is < thres_max. Then set it to the previous intensity.\n",
    "Red/Infrared (noise channel): Start increasing intensity and keep it in some range, i.e. as long as DC is > thres_min, STOP!.\n",
    "'''\n",
    "# Gives root access to the Python script.\n",
    "#\"D4:F7:FA:85:56:AA\",\"DD:44:77:53:96:0E\",\"F0:77:95:41:44:62\",\"D2:67:36:A2:9E:9F\"\n",
    "#bigmacs = np.array([\"CE:29:1F:E7:26:8B\", \"FE:23:64:32:DF:41\", \"D4:F7:FA:85:56:AA\"]);\n",
    "bigmacs = np.array([\"c3:68:c4:59:2f:19\"]);\n",
    "numSensors = 1\n",
    "\n",
    "mac_address1 = bigmacs[0]\n",
    "\n",
    "\n",
    "if os.geteuid() != 0:\n",
    "    # os.execvp() replaces the running process, rather than launching a child\n",
    "    # process, so there's no need to exit afterwards. The extra \"sudo\" in the\n",
    "    # second parameter is required because Python doesn't automatically set $0\n",
    "    # in the new process.\n",
    "    # os.execvp(\"sudo\", [\"sudo\"] + sys.argv)\n",
    "    pass\n",
    "\n",
    "def write_uint16(data, value, index):\n",
    "    \"\"\" Write 16bit value into data string at index and return new string \"\"\"\n",
    "    data = data.decode('utf-8')  # This line is added to make sure both Python 2 and 3 works\n",
    "    return '{}{:02x}{:02x}{}'.format(\n",
    "                data[:index*4],\n",
    "                value & 0xFF, value >> 8,\n",
    "                data[index*4 + 4:])\n",
    "\n",
    "def write_uint8(data, value, index):\n",
    "    \"\"\" Write 8bit value into data string at index and return new string \"\"\"\n",
    "    data = data.decode('utf-8')  # This line is added to make sure both Python 2 and 3 works\n",
    "    return '{}{:02x}{}'.format(\n",
    "                data[:index*2],\n",
    "                value,\n",
    "                data[index*2 + 2:])\n",
    "\n",
    "# The base UUID used in bluetooth services in all hardware setups.\n",
    "def MotionSense_UUID(val):\n",
    "    \"\"\" Adds base UUID and inserts value to return motionSenseHRV UUID \"\"\"\n",
    "    return UUID(\"DA39%04X-1D81-48E2-9C68-D0AE4BBD351F\" % val)\n",
    "\n",
    "# Definition of all UUID used by motionSenseHRV\n",
    "BATTERY_SERVICE_UUID = 0xADF0\n",
    "BATTERY_CHAR_UUID    = 0x2A19\n",
    "\n",
    "MOTION_SERVICE_UUID  = 0x5D22\n",
    "MOTION_ACCELERO_CHAR_UUID = 0xC922\n",
    "CCCD_UUID            = 0x2902\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Notification handles used in notification delegate\n",
    "m_motion_handle = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MotionServiceHRV():\n",
    "    data_samples = []\n",
    "    \"\"\"\n",
    "    Environment service module. Instance the class and enable to get access to the Environment interface.\n",
    "    \"\"\"\n",
    "    serviceUUID         =   MotionSense_UUID(MOTION_SERVICE_UUID)\n",
    "    motion_char_uuid   =   MotionSense_UUID(MOTION_ACCELERO_CHAR_UUID)\n",
    "\n",
    "    def __init__(self, periph):\n",
    "        self.periph             = periph\n",
    "        self.motion_service     = None\n",
    "        self.motion_char        = None\n",
    "        self.motion_cccd        = None\n",
    "\n",
    "    def enable(self):\n",
    "        global m_motion_handle\n",
    "\n",
    "        \"\"\" Enables the class by finding the service and its characteristics. \"\"\"\n",
    "        if self.motion_service is None:\n",
    "            self.motion_service = self.periph.getServiceByUUID(self.serviceUUID)\n",
    "        if self.motion_char is None:\n",
    "            self.motion_char = self.motion_service.getCharacteristics(self.motion_char_uuid)[0]\n",
    "            m_motion_handle = self.motion_char.getHandle()\n",
    "            print(m_motion_handle)\n",
    "            print(self.motion_char.getDescriptors())\n",
    "            self.motion_cccd = self.motion_char.getDescriptors(forUUID=CCCD_UUID)[0]\n",
    "\n",
    "    def set_motion_notification(self, state):\n",
    "        if self.motion_cccd is not None:\n",
    "            if state == True:\n",
    "                self.motion_cccd.write(b\"\\x01\\x00\", True)\n",
    "            else:\n",
    "                self.motion_cccd.write(b\"\\x00\\x00\", True)\n",
    "\n",
    "\n",
    "    def disable(self):\n",
    "        self.set_motion_notification(False)\n",
    "\n",
    "class MotionSenseHRVplus(Peripheral):\n",
    "    def __init__(self, addr):\n",
    "        Peripheral.__init__(self, addr, addrType=ADDR_TYPE_RANDOM)\n",
    "\n",
    "        # Thingy configuration service not implemented\n",
    "        self.motionHRVplus = MotionServiceHRV(self)\n",
    "\n",
    "class MyDelegate(DefaultDelegate):\n",
    "    ts = None;\n",
    "\n",
    "    def handleNotification(self, hnd, data):\n",
    "        if (hnd == m_motion_handle):\n",
    "            self.ts = round(time.time()*1000)\n",
    "\n",
    "            #print(data)\n",
    "            data2Format = struct.unpack('>BH', data)\n",
    "            #print(\"printing unformatted {}\".format(binascii.b2a_hex(data).decode('utf-8')))\n",
    "            #print(binascii.b2a_hex(data).decode('utf-8'))\n",
    "            print(\"data={}, counter={}\".format( data2Format[0], data2Format[1],))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ScanDelegate(DefaultDelegate):\n",
    "\n",
    "    def handleDiscovery(self, dev, isNewDev, isNewData):\n",
    "        print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()), dev.addr, dev.getScanData())\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def main():\n",
    "    scanner = Scanner().withDelegate(ScanDelegate())\n",
    "\n",
    "# listen for ADV_IND packages for 10s, then exit\n",
    "    devices = scanner.scan(5.0, passive=True)\n",
    "\n",
    "    for dev in devices:\n",
    "         uu=dev.getScanData()\n",
    "         if len(uu) >1:\n",
    "              if uu[1][2] == \"MotionSenseHRV3\":\n",
    "                   print(dev.addr)\n",
    "                   mac_address1 = dev.addr\n",
    "    e1 = 'sudo echo 8 > /sys/kernel/debug/bluetooth/hci0/conn_min_interval'\n",
    "    e2 = 'sudo echo 13 > /sys/kernel/debug/bluetooth/hci0/conn_max_interval'\n",
    "    subprocess.call(e1,shell=True);\n",
    "    subprocess.call(e2,shell=True);\n",
    "\n",
    "   # mac_address1 = \"df:8f:a1:f5:23:84\"\n",
    "    #msHrv1 = MotionSenseHRVplus(mac_address1)\n",
    "    #mac_address2 = bigmacs[int(sys.argv[1])] #\"D4:F7:FA:85:56:AA\"\n",
    "    #mac_address1 = bigmacs[int(sys.argv[2])] #\"DD:44:77:53:96:0E\"\n",
    "    #mac_address2 = sys.argv[1]\n",
    "\n",
    "    msHrv1 = MotionSenseHRVplus(mac_address1)\n",
    "    print('Connected sensor 1 ...')\n",
    "\n",
    "    md1 = MyDelegate()\n",
    "\n",
    "\n",
    "    msHrv1.motionHRVplus.enable()\n",
    "\n",
    "    msHrv1.setDelegate(md1)\n",
    "\n",
    "\n",
    "    msHrv1.motionHRVplus.set_motion_notification(True)\n",
    "    #msHrv1.motionHRVplus.set_magneto_notification(True)\n",
    "\n",
    "\n",
    "\n",
    "    while 1:\n",
    "        data = msHrv1.waitForNotifications(5)\n",
    "        print('Now collecting measurements...')\n",
    "        data_samples.append(data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code collects our data and puts all of the 16 float values into an array, whitch is then put into another array representing the sample number. This is then fed into our decoder to output as a reconstrcted ppg signal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    reconstrct_sig = np.array(data)\n",
    "    #decoder.predict()\n",
    "\n",
    "#this is the last portion of code I need to add, just the full end to end prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This concludes the end of the tutorial. If you had issues with the tutorial, or you encountered a bug/issue, please submit an issue request in this github repository or on our website, and we will try to fix the bug and/or help you as soon as possible."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZwF0gRz0eOx6JSY/eOPP9",
   "name": "AE_model_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}