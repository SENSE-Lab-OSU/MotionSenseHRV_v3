{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC7jZlOZGcUs"
   },
   "source": [
    "# **Task Specific Autoencoder enabled by Tensorflow-Lite Micro end-to-end Tutorial-Deploying to NRF 5340 and OSU Motionsense HRV**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPjMu0MEY9DJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PggJ5kyHGdb1"
   },
   "source": [
    "In this notebook we will show you how to deploy a tensorflow lite micro autoencoder to a cortex-M microcontroller such as the nrf5340 in order to trasmit photoplethysmogram (ppg) IR signals to an mobile device for smooth Heart Rate (HR) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KDoF6b9ZLAI"
   },
   "source": [
    "In the last few years, the power of stronger processors, combined with new techniques for reducing size and cost of machine learning models (a process called quantization) has allowed ai to come all the way down to the internet of things (IoT) level. This is the level of microcontrollers and sensor systems. In other words, with recent proper techniques we can now deploy machine learning on really tiny devices that require very little power, such as watches, security cameras, and wearable medical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybR1g73pbj75"
   },
   "source": [
    "There are many benefits to being able to employ machine learning on IoT devices, and the possibilities are endless. In this tutorial, however, we will be taking a look at deploying a *task specific autoencoder*. Autoencoders are a type of neural network that, given an input, will reduce or compress the size to be very small. Then, they attempt to learn from this compressed signal by trying to recreate it as the output. This allows the model to learn what signals/features are key and should be preserved, and what should be ignored. In our case, we will be using this autoenocoder for feature-preserving compression. That is, we will encode data coming from a ppg sensor and use this to send over Bluetooth Low Energy (BLE) so that this data can be further analyzed by higher caliber and deeper networks with things such as Smooth Heart Rate prediction (HR). BLE is very low-power, and as such, cannot transmit large quanities of data easily such as a full ppg signal. As a result of this, the Autoencoder allows the data to be compressed to such small sizes that only the features needed for learning are kept, and this is a small enough size to be sent over through BLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7m5KuKdt3h"
   },
   "source": [
    "Let us start working on the project itself. You will need:\n",
    "\n",
    "\n",
    "1.   An OSU MotionSenseHRV chip\n",
    "2.   Anaconda or Miniconda installed (See https://docs.anaconda.com/anaconda/install/)\n",
    "\n",
    "After step 2, you'll have conda package manager (for python packages) either through your terminal (Linux, macOS) or through Anaconda Prompt (Windows). Open it in the \"tutorials\" folder as admin/sudo and run the following conda command. These will install all python dependencies in an environment named tf_MSHRV3_AEHR and activate it. Then, re-open this notebook from inside tf_MSHRV3_AEHR environment by simply executing \"jupyter notebook AE_model_tutorial.ipynb\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvCAiyOVPc_v"
   },
   "source": [
    "```\n",
    "conda env create -f environment.yml\n",
    "conda activate tf_MSHRV3_AEHR\n",
    "jupyter notebook AE_model_tutorial.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to Download the Data via. an automatic python script. Run this only once.\n",
    "\n",
    "#from data import get_data\n",
    "#get_data.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjMv94yRWD2T"
   },
   "source": [
    "**Make sure parameters and sample data are in the correct folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcPP2NEjVS17"
   },
   "source": [
    "We will start by getting the necessary model parameters and sample data in order to first test the model. To do this, run the get_data.py file in the repository, or run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zZQ83gIGcl0"
   },
   "outputs": [],
   "source": [
    "# TensorFlow is an open source machine learning library\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras is TensorFlow's high-level API for deep learning\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Numpy is a math library\n",
    "import numpy as np\n",
    "# Pandas is a data manipulation library \n",
    "import pandas as pd\n",
    "# Scipy is a signal-processing library\n",
    "from scipy.signal import detrend\n",
    "# Matplotlib is a graphing library\n",
    "import matplotlib.pyplot as plt\n",
    "# Math is Python's math library\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#import some custom utility functions\n",
    "from utils import make_data_pipe, Rpeak2HR, sliding_window_fragmentation\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "\n",
    "path_prefix='./data/pre-training/'\n",
    "weights_path_prefix='./data/post-training/model_weights' #for saved weights\n",
    "\n",
    "val_files=[path_prefix+'2019092801_3154_clean.csv']\n",
    "test_files=[path_prefix+'2019092820_5701_clean.csv']\n",
    "win_len=8 #the window length, in seconds\n",
    "step=1 #in n_samples\n",
    "Fs_pks=100 #in Hz\n",
    "\n",
    "\n",
    "#TODO: this code will not run here for obvious reasons. In the public repository, will these files be located in the directory?\n",
    "#If so I will provide a paragraph to make sure it is in the right settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we've configured our paths and imports, we should start by first loading our training data for the model, located in the repository as a spreadsheet csv file of a sample recorded ppg signal through time. We do this by using pd.readcsv(), whitch gets our data, and then we reformat and combine using numpy concatenations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uYZnMSSXlkB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_data(path,val_files=[],test_files=[],\n",
    "                   win_len=8,step=1,Fs_pks=100):\n",
    "\n",
    "    def get_clean_ppg_and_ecg(files):\n",
    "        list_clean_ppg=[];list_arr_pks=[]\n",
    "        for i in range(len(files)):\n",
    "            df=pd.read_csv(files[i],header=None)\n",
    "            arr=df.values\n",
    "            if 'clean' in files[i]:\n",
    "                arr[:,41:45]=(detrend(arr[:,41:45].reshape(-1),0,'constant')\n",
    "                                ).reshape((-1,4))\n",
    "                list_clean_ppg+=[np.concatenate([arr[:,29:30],arr[:,41:45]],\n",
    "                                    axis=-1),arr[:,30:31],arr[:,39:40],\n",
    "                                arr[:,40:41]]\n",
    "                list_arr_pks+=[arr[:,45:49].reshape(-1)]\n",
    "        return list_clean_ppg,list_arr_pks\n",
    "    files=glob.glob(path+'*.csv')\n",
    "    #files=[fil for fil in files if 'WZ' in fil] #get wenxiao's data\n",
    "    #separate val and test files\n",
    "    s3=set(files);s4=set(val_files+test_files)\n",
    "    files_2=list(s3.difference(s4))\n",
    "    #files_2=[files_2[0]]\n",
    "    #files_2=[fil for fil in files if not((val_names[0] in fil))]\n",
    "    list_clean_ppg,list_arr_pks=get_clean_ppg_and_ecg(files_2)\n",
    "\n",
    "    dsample_factr=4\n",
    "    Fs_pks=int(Fs_pks/dsample_factr)\n",
    "    win_len=win_len*Fs_pks\n",
    "\n",
    "    list_r_pk_locs=[np.arange(len(arr_pks))[arr_pks.astype(bool)] for\n",
    "                    arr_pks in list_arr_pks]\n",
    "\n",
    "    #get nearest dsampled idx\n",
    "    list_r_pk_locs_dsampled=[np.round(r_pk_locs/dsample_factr).astype(int) for\n",
    "                             r_pk_locs in list_r_pk_locs]\n",
    "\n",
    "    list_arr_pks_dsampled=[]\n",
    "    for j in range(len(list_arr_pks)):\n",
    "        arr_pks_dsampled=np.zeros([len(list_clean_ppg[4*j]),1])\n",
    "        #check & correct for rare rounding up issue in the last element\n",
    "        if list_r_pk_locs_dsampled[j][-1]==len(arr_pks_dsampled):\n",
    "            list_r_pk_locs_dsampled[j][-1]-=1\n",
    "        arr_pks_dsampled[list_r_pk_locs_dsampled[j]]=1\n",
    "        list_arr_pks_dsampled.append(arr_pks_dsampled)\n",
    "    #print([len(ppg) for ppg in list_arr_pks_dsampled])\n",
    "\n",
    "\n",
    "    list_HR=[dsample_factr*[Rpeak2HR(arr_pks,win_len,step,Fs_pks)]\n",
    "             for arr_pks in list_arr_pks_dsampled]\n",
    "    list_HR=sum(list_HR,[])\n",
    "    #list_HR=[HR[::dsample_factr] for HR in list_HR]\n",
    "\n",
    "    return list_clean_ppg,list_HR\n",
    "\n",
    "\n",
    "\n",
    "#input_list,output_list=[],[]\n",
    "list_sigs,list_HR=get_train_data(path_prefix,val_files,test_files,win_len,\n",
    "                                           step,Fs_pks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**import training data from csv file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FA0yDscf9ek"
   },
   "source": [
    "**Convert and Visualize Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPaS6RLufYqH"
   },
   "source": [
    "We will feed the data to the network using tensorflow's [tf.data](https://www.tensorflow.org/guide/data) pipeline which comes with lots of benefits (check out the link to learn more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx6Qza8Vf1jG"
   },
   "outputs": [],
   "source": [
    "#Pre-process data\n",
    "Fs_ppg=25 #Hz\n",
    "sample_win_len,step_size=win_len*Fs_ppg,2*Fs_ppg\n",
    "HR_win_len=sample_win_len\n",
    "ppg_win_len=sample_win_len+HR_win_len\n",
    "model_sigs_in,model_HR_out=[],[]\n",
    "for j in range(len(list_HR)):\n",
    "    #fragmenting our signal into 8 second windows\n",
    "    ppg,HR=list_sigs[j][:,0:1],list_HR[j]\n",
    "    ppg=sliding_window_fragmentation([ppg],ppg_win_len,step_size)\n",
    "    HR=sliding_window_fragmentation([HR],HR_win_len,step_size)\n",
    "    #chaining it all together\n",
    "    model_sigs_in.append(ppg)\n",
    "    model_HR_out.append(HR[:len(ppg)])\n",
    "\n",
    "#flattening the data to it is in a straight line\n",
    "model_sigs_in=np.concatenate(model_sigs_in,axis=0)\n",
    "model_HR_out=np.concatenate(model_HR_out,axis=0)\n",
    "model_in=model_sigs_in[:,:,0] \n",
    "model_out=model_HR_out[:,:,0]\n",
    "print(model_in.shape,model_out.shape)\n",
    "\n",
    "#Visualize our PPG signal\n",
    "idx=1\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.title('A sample PPG and HR')\n",
    "plt.plot(model_in[idx,:])\n",
    "plt.ylabel('PPG')\n",
    "plt.grid(True)\n",
    "plt.subplot(212)\n",
    "plt.plot(model_out[idx,:])\n",
    "plt.ylabel('HR (BPS)')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Sample No.')\n",
    "#partition\n",
    "val_perc=0.14\n",
    "val_idx=int(val_perc*len(model_in))\n",
    "\n",
    "#get our final formats and make the data pipe for our model\n",
    "val_data_AE=[model_in[0:val_idx,:],model_in[0:val_idx]]\n",
    "train_data_AE=[model_in[val_idx:,:],model_in[val_idx:]]\n",
    "val_data_e2e=[model_in[0:val_idx,:],model_out[0:val_idx]]\n",
    "train_data_e2e=[model_in[val_idx:,:],model_out[val_idx:]]\n",
    "\n",
    "train_ds_AE=make_data_pipe(train_data_AE,batch_size=32,shuffle=True)\n",
    "val_ds_AE=make_data_pipe(val_data_AE,batch_size=128,shuffle=False)\n",
    "train_ds_e2e=make_data_pipe(train_data_e2e,batch_size=32,shuffle=True)\n",
    "val_ds_e2e=make_data_pipe(val_data_e2e,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fWoLh6FGbYl"
   },
   "source": [
    "**Loading/Training the model**\n",
    "\n",
    "Now that we have the necessary data in order to properly load, train and test our model, we will start by importing it and then pre processing it.\n",
    "\n",
    "We begin by constructing our machine learning model. Our target network is an AutoEncoder that will encode 8 seconds of a ppg signal into a single 1x16 latent representation (or \"code\"). The layers in between are responsible for the feature-selective compression. If you look at the model, we start with our input, then  convert it into a 256 x 1 layer, with a rectified linear activation function. This is then shrunk to a 64 layer, and then finally our 16 layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5lnHXtYlzgm"
   },
   "outputs": [],
   "source": [
    "def create_model_AE(in_shape,latent_shape=(16,),encoder=None,decoder=None):\n",
    "    #We will be creating the AE_model here\n",
    "    if encoder is None:\n",
    "        encoder_in = layers.Input(shape=in_shape,name=\"encoder_in\")\n",
    "        x = layers.Dense(256,activation='relu')(encoder_in)\n",
    "        x = layers.Dense(64,activation='relu')(x)\n",
    "        encoder_out = layers.Dense(latent_shape[0],activation='relu')(x)\n",
    "        encoder = keras.Model(encoder_in, encoder_out, name=\"encoder\")\n",
    "\n",
    "    #our encoder is what will be on the edge-device. Since we also need to uncompress,\n",
    "    #or decode it, we must also create a decoder, and then combine them to together in order to train them\n",
    "    if decoder is None:\n",
    "        decoder_in = layers.Input(shape=latent_shape,name=\"decoder_in\")\n",
    "        x = layers.Dense(64,activation='relu')(decoder_in)\n",
    "        x = layers.Dense(256,activation='relu')(x)\n",
    "        decoder_out = layers.Dense(in_shape[0])(x)\n",
    "        decoder = keras.Model(decoder_in, decoder_out, name=\"decoder\")\n",
    "    \n",
    "    AE_in = keras.Input(shape=in_shape, name=\"AE_in\")\n",
    "    z = encoder(AE_in)\n",
    "    sig_hat = decoder(z)\n",
    "    model_AE = keras.Model(AE_in, sig_hat, name=\"AE\")\n",
    "    return encoder,decoder,model_AE\n",
    "\n",
    "#now we will use this function to make the AE model\n",
    "encoder,decoder,model_AE = create_model_AE(model_in.shape[1:],\n",
    "                                           latent_shape=(16,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have our auto encoder, let's also connect our smooth heart rate prediction model to the autoencoder to that it can be optimized to be task specific. the HR model will append onto the decoder and find the heart rate from the reconstructed ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing PPG to HR Neural Network model called model_HR for smooth Heart Rate Prediction from one channel PPG signal\n",
    "def create_model_HR(in_shape,HR_win_len=200):\n",
    "    expand_dims = layers.Lambda(lambda x: tf.expand_dims(x,axis=-1), \n",
    "                                name='expand_dims')\n",
    "    #RNN model via. Functional API\n",
    "    rnn = layers.GRU(64, return_sequences=True, return_state=True)\n",
    "    sig_in = layers.Input(shape=in_shape)\n",
    "    x = expand_dims(sig_in)\n",
    "    _, final_state=rnn(x[:,:HR_win_len,:]) #warm-up RNN\n",
    "    rnn_out, _ = rnn(x[:,HR_win_len:,:],initial_state=final_state)\n",
    "    HR_hat=layers.Conv1D(filters=1,kernel_size=1, strides=1,padding='same',\n",
    "                         activation=None,name='Conv_{}'.format(1))(rnn_out)\n",
    "    HR_hat=layers.Flatten()(HR_hat)\n",
    "    model = keras.Model(sig_in, HR_hat, name='model_HR')\n",
    "    return model\n",
    "\n",
    "#% Load HR model\n",
    "model_HR = create_model_HR(model_in.shape[1:],HR_win_len)\n",
    "weights_dir_HR=weights_path_prefix+'/sig2HR/checkpoints'\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_HR)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_HR.load_weights(latest_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We then stack the model_AE and model_HR together in a model called model_e2e (end2end).\n",
    "def create_model_e2e(in_shape,encoder,decoder,model_HR):\n",
    "    #Put the models together\n",
    "    e2e_in = keras.Input(shape=in_shape, name=\"e2e_in\")\n",
    "    HR_hat=model_HR(decoder(encoder(e2e_in)))\n",
    "    model_e2e = keras.Model(e2e_in, HR_hat, name=\"e2e\")\n",
    "    return model_e2e\n",
    "\n",
    "# Make e2e model\n",
    "model_e2e = create_model_e2e(model_in.shape[1:],encoder,decoder,model_HR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's inspect our model and, finally, train it. The training process may take a while depending on your PC hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model_in.shape is the input format\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())\n",
    "print(model_AE.summary())\n",
    "print(model_HR.summary())\n",
    "print(model_e2e.summary())\n",
    "\n",
    "plot_model=False\n",
    "file_path='./data/figures'\n",
    "os.makedirs(file_path,exist_ok=True)\n",
    "\n",
    "if plot_model:\n",
    "    tf.keras.utils.plot_model(model_AE,to_file=file_path+'/AE.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)\n",
    "    tf.keras.utils.plot_model(model_e2e,to_file=file_path+'/e2e.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#compiling the model prepares it for training\n",
    "model_AE.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "#we will also set a directory to save our trained weights once they are finished\n",
    "weights_dir_AE=weights_path_prefix+'/AE/checkpoints'\n",
    "os.makedirs(weights_dir_AE,exist_ok=True)\n",
    "ckpt_filepath_AE=weights_dir_AE+'/cp.ckpt'\n",
    "#callbacks\n",
    "callbacks=[]\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_AE,\n",
    "                    save_weights_only=True,save_best_only=True,\n",
    "                    monitor=\"val_loss\",mode='min'))\n",
    "\n",
    "# Train Simple AE for reconstruction\n",
    "model_AE.fit(train_ds_AE,epochs=400,validation_data=val_ds_AE,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load best AE ckpt\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_AE)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_AE.load_weights(latest_ckpt)\n",
    "\n",
    "# Train End2End model\n",
    "model_e2e.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "weights_dir_e2e=weights_path_prefix+'/AE/checkpoints_e2e'\n",
    "os.makedirs(weights_dir_e2e,exist_ok=True)\n",
    "ckpt_filepath_e2e=weights_dir_e2e+'/cp.ckpt'\n",
    "\n",
    "#callbacks\n",
    "callbacks=[]\n",
    "\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e,\n",
    "                save_weights_only=True,save_best_only=True,\n",
    "                monitor=\"val_loss\",mode='min'))\n",
    "\n",
    "# Train\n",
    "model_e2e.fit(train_ds_e2e,epochs=100,validation_data=val_ds_e2e,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTx0MUtQnGC1"
   },
   "source": [
    "**Prepare for Quanitization aware training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXBQMqBMnNuc"
   },
   "source": [
    "Now that we have our model ready, we should now use something called quantization aware training (QAT), that will train our network in preparation to be converted to a lighter format. Integer Quantization is the process of converting parameters (weights, bias, and activations) in our network to 8 bit integer values, used because 8bit integer values are both faster and lighter in terms of storage, whitch is very helpful for microcontrollers. While QAT is not the quantization process itself, This training will optimize our weights to the 8bit model and fine tune them to prepare for this conversion process. It does this by simulating what the model would run like if it used integers, and then optimizing these pre-converted numbers to minized the loss function AFTER it has been trained. Note that we have not actually converted the numbers by involking QAT, we have just prepared the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba-PeGYWn0J5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmopt\n",
    "\n",
    "#Load best e2e ckpt\n",
    "#latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e)\n",
    "#print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "#model_e2e.load_weights(latest_ckpt)\n",
    "\n",
    "#performing generation of the quantization aware model of the encoder\n",
    "quan_model_generator = tfmopt.quantization.keras.quantize_model\n",
    "encoder_q = quan_model_generator(encoder) #quantization aware encoder\n",
    "decoder_noq = keras.models.clone_model(decoder)\n",
    "decoder_noq.set_weights(decoder.get_weights())\n",
    "decoder_noq.trainable=False #Freeze decoder for now\n",
    "    \n",
    "# Make the AE model, for quantization of it later on\n",
    "_,_,model_AE_q = create_model_AE(model_in.shape[1:],\n",
    "latent_shape=(16,),encoder=encoder_q,decoder=decoder_noq)\n",
    "# Make e2e model\n",
    "model_e2e_q = create_model_e2e(model_in.shape[1:],encoder_q,decoder_noq,model_HR)\n",
    "\n",
    "\n",
    "#model_in.shape is the input format, you should see (0, 16)\n",
    "print(encoder_q.summary())\n",
    "\n",
    "if plot_model:\n",
    "    tf.keras.utils.plot_model(model_e2e_q,to_file=file_path+'/e2e_q.png', \n",
    "    dpi=200, show_shapes=True, show_layer_names=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Train QAT model**\n",
    "\n",
    "The QAT version of our model has now been configured properly and is ready to 8-bit optimize, by training. We train this model in the same fashion that we trained our original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_e2e_q.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "#we set the directory to our pretrained path\n",
    "weights_dir_e2e_q = weights_path_prefix+'/AE/checkpoints_e2e_q'\n",
    "os.makedirs(weights_dir_e2e_q,exist_ok=True)\n",
    "ckpt_filepath_e2e_q=weights_dir_e2e_q+'/cp.ckpt'\n",
    "\n",
    "callbacks=[]\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_filepath_e2e_q,\n",
    "                save_weights_only=True,save_best_only=True,\n",
    "                monitor=\"val_loss\",mode='min'))\n",
    "# Train\n",
    "model_e2e_q.fit(train_ds_e2e,epochs=100,validation_data=val_ds_e2e,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgqBJSCzoryb"
   },
   "source": [
    "**Compare QAT results with normal model**\n",
    "We also should make sure that most of our accuracy is preserved during this conversion process. Here we check only using validation set but we encourage and leave this verification, using the test data, as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryFFx7HAorCN"
   },
   "outputs": [],
   "source": [
    "#Load best e2e_q checkpoint, training by the model\n",
    "latest_ckpt = tf.train.latest_checkpoint(weights_dir_e2e_q)\n",
    "print('Loading model from ckpt {}'.format(latest_ckpt))\n",
    "model_e2e_q.load_weights(latest_ckpt)\n",
    "\n",
    "baseline_model_mse = model_e2e.evaluate(val_ds_e2e, verbose=1)\n",
    "q_aware_model_mse = model_e2e_q.evaluate(val_ds_e2e, verbose=1)\n",
    "\n",
    "print('Baseline val mse:', baseline_model_mse)\n",
    "print('Quant val mse:', q_aware_model_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APir38KsqMIN"
   },
   "source": [
    "**Quantize the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc_1HxR6qTbT"
   },
   "source": [
    "We are now fully ready to convert the model into it's lightweight component, an 8 bit fixed point version of our model. Normally, in machine learning models we use a floating point 32 type of number, whitch is a number that supports decimal points and has a high precision amount, meaning you can type a lot of numbers into it. 8 bit integers only support up to 256 possible numbers, and as such, are not as accurate, but are much faster to run. After running this code, a model named according to the string in \"filename\". Quantization is typically done by representing the number by this format: Real number = most_significant_digits_of_real_number*scaling_factor. This is the esense of what the converter is doing, though there are many optimizations you can make to ensure this runs with optimal accuracy. You can do this by modifying the attributes of the converter objects, as you can see in the code below. However, since we have employed Quantization Awareness training, most of these settings are automated, and we just need to make sure only one flag, converter.optimizations, is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vw0wwQXdq3oP"
   },
   "outputs": [],
   "source": [
    "file_name = \"tf_lit2_encoder.tflite\"\n",
    "#prepare to convert the encoder model for deeployment by getting our converter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(encoder_q)\n",
    "\n",
    "\n",
    "#the following optimizations are already configured by QAT:\n",
    "#converter.representitive_dataset\n",
    "#converter.target_spec\n",
    "#converter.inference_input_type and converter.inference_output_type\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "#this converts the model to it's 8 bit weight, bias, and activation form\n",
    "encoder_tf_lite = converter.convert()\n",
    "\n",
    "#we now have our model, and we can save it to our directory for deployment\n",
    "open(file_name, \"wb\").write(encoder_tf_lite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjHfb6pfrFHQ"
   },
   "source": [
    "**Compare our model to the original**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA8oY_fhthJ8"
   },
   "source": [
    "To make sure our model works, we will now test it with the baseline model and compare mean-sqaure error values to measure how accurate they are. In Tensorflow, you can to this with model.evaluate, but because we are now also working with a tensorflow lite model format we need to pass induvidual values to it through a loop and then stack the values with numpy. Autoencoders work just like any other network in that you can compare the results of the network's output with the desired result, in this case our real ppg signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwh43oVM6PkC"
   },
   "outputs": [],
   "source": [
    "test_tf_lite_model = tf.lite.Interpreter(\"tf_lit2_encoder.tflite\")\n",
    "#you can change the name of this model in the program by renaming the \"file_name\" variable and then\n",
    "#replaceing the above string with your file_name variable\n",
    "test_tf_lite_model.allocate_tensors()\n",
    "#the model involking actually works the same way in tflite micro, the only difference is the language\n",
    "input_tensor = test_tf_lite_model.get_input_details()[0][\"index\"]\n",
    "output_tensor = test_tf_lite_model.get_output_details()\n",
    "\n",
    "repre_data_set = np.array(val_data_AE[1])\n",
    "np.random.shuffle(repre_data_set)\n",
    "\n",
    "result_difference = 0\n",
    "results = []\n",
    "sample_test_num = 3493\n",
    "first_time = False\n",
    "# Run the model's interpreter for each value and store the results in arrays\n",
    "for i in range(0, sample_test_num):\n",
    "    calibrat_data = repre_data_set[i]\n",
    "    calibrat_data = np.reshape(calibrat_data, (1, 400))\n",
    "    tensor_calibrat = tf.convert_to_tensor(calibrat_data, dtype=tf.float32)\n",
    "    test_tf_lite_model.set_tensor(input_tensor, tensor_calibrat)\n",
    "    test_tf_lite_model.invoke()\n",
    "    output_data = test_tf_lite_model.get_tensor(output_tensor[0]['index'])\n",
    "    normal_model_output = 0\n",
    "    if not first_time:\n",
    "        result = np.array(output_data)\n",
    "        first_time = True\n",
    "        result_parent = np.array(normal_model_output)\n",
    "    else:\n",
    "        result = np.vstack((result, output_data))\n",
    "        result_parent = np.vstack((result_parent, normal_model_output))\n",
    "\n",
    "    #now we have the data from both sets encoder models. All we need to do is pass them through data\n",
    "\n",
    "    #now we are just taking the average of the difference of each value in the output tensor and printing this\n",
    "    result_arr = np.subtract(normal_model_output, output_data)\n",
    "    result_arr = np.absolute(result_arr)\n",
    "    result_difference = np.mean(result_arr)\n",
    "    #we are taking a mean here to compare the result_difference)\n",
    "    results.append(float(result_difference))\n",
    "\n",
    "final_diff = np.mean(results)\n",
    "result_parent = encoder.predict(repre_data_set)\n",
    "decoder.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mse\"])\n",
    "print(\"the mean ouput loss across\", sample_test_num, \"runs is \", final_diff)\n",
    "tflite_loss = decoder.evaluate(result, repre_data_set)\n",
    "parent_loss = decoder.evaluate(result_parent, repre_data_set)\n",
    "model_AE.evaluate(repre_data_set, repre_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64zKtXlvMHWY"
   },
   "source": [
    "**Convert the model to a Tflite micro model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjlVHTr5NKtu"
   },
   "source": [
    "Now that we have our full tensorflow lite model, we'll need to convert this model into a tf-lite-micro file so that it can be deployed to a micro-controller. You can do this by opening up Linux and running the following commands in the terminal. Make sure you replace the MODEL_TFLITE and MODEL_TFLITE_MICRO with the tflite file name and the converted name you want the file to be, respectively. For Windows users, xxd package is available in Vim for Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install xxd if it is not available\n",
    "!apt-get update && apt-get -qq install xxd\n",
    "\n",
    "### Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Apb78TZSZIG"
   },
   "source": [
    "### **Load a tensorflow nrf-project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a--Qwq0XS6VO"
   },
   "source": [
    "This is where our project gets a little specific. We are working on the  MotionSense_v3 Device specific instructions right now and will update this tutorial soon. So Stay Tuned!!..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZwF0gRz0eOx6JSY/eOPP9",
   "name": "AE_model_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
